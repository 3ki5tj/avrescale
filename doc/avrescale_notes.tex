%\documentclass[12pt]{article}
\documentclass[preprint]{revtex4-1}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
\usepackage{hyperref}

\hypersetup{
  colorlinks,
  linkcolor={red!30!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{64,0,42}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}
\newcommand{\answer}[1]{{\color{DarkBlue}\footnotesize \textsc{Answer.} #1}}
\newcommand{\summary}[1]{{\color{DarkPurple}\footnotesize \textsc{Summary.} #1}}



\begin{document}



\title{Notes on ``Adaptive velocity scaling for an asymptotic
microcanonical ensemble''}
\author{}
%\date{\vspace{-7ex}}

\maketitle



\subsection{Derivative of the bulk entropy}



The bulk
or Gibbs entropy\cite{cagin1988, haile, dunkel2014},
$S_\Phi \equiv k_B \, \ln \Phi(E)$,
is based on the hypervolume enclosed by an isoenergetic surface,
Eq. \eqref{eq:PhiE}.
%
This definition results in a temperature, Eq. \eqref{eq:TE_K}
that agrees with the commonly-used definition,
Eq. \eqref{eq:K_canon}.
%
To derive this result,
consider a vector field $\mathbf w$
that satisfies $\nabla \cdot \mathbf w = 1$.
Then
%
\begin{align*}
  \Phi(E)
  &=
  \int
    \left(
      \nabla \cdot \mathbf w
    \right)
    \,
    \Theta\bigl(
      E - H(\mathbf x)
    \bigr)
    \, d\mathbf x
  \\
  &=
  \int
    \left( \mathbf w \cdot \nabla H \right)
    \,
    \delta\bigl(
      E - H(\mathbf x)
    \bigr)
    \, d\mathbf x
  \\
  &= \langle \mathbf w \cdot \nabla H \rangle_E
    \,
    \Omega(E)
  .
\end{align*}
%
A convenient candidate of $\mathbf w$ is
$\mathbf w = \mathbf p / N_f$.
%
Then,
$\mathbf w \cdot \nabla H = 2 \, K / N_f$,
and
%
\begin{equation}
  k_B \, T_\Phi(E)
  =
  \frac{ \Phi(E) } { \Phi'(E) }
  =
  \frac{ \Phi(E) } { \Omega(E) }
  =
  \frac{ 2 \, \langle K \rangle_E } { N_f }
  .
  \label{eq:Tbulk}
\end{equation}
%
Alternatively, this result may be derived using
Laplace transform\cite{pearson1985, haile}.
%
%Further
%%
%\begin{align}
%  k_B \, T'_\Phi(E)
%  &=
%  1 - k_B \, T_\Phi(E) \, \beta_\Omega(E)
%  \label{eq:dTbulk}
%  \\
%  &=
%  k_B \, T_\Phi(E)
%  \left\langle
%  K^{-1}
%  \right\rangle_E
%  +
%  \left\langle
%  \Delta \left( K^{-1} \right)
%  \Delta K
%  \right\rangle_E
%  ,
%  \notag
%\end{align}
%%
%which can be approximated as a trajectory average
%%
%\begin{equation}
%  k_B \, T'_\Phi(E^*)
%  =
%  k_B \, T^* \overline{ K_t^{-1} }
%  +
%  \mathrm{Cov}\left(
%    K_t, K_t^{-1}
%  \right)
%  .
%  \notag
%\end{equation}
%%
%From Eq. \eqref{eq:dTbulk},
%it is also clear that the surface microcanonical temperature,
%$T_\Omega(E)$,
%would be slightly greater than the bulk one, $T_\Phi(E)$.
%
%\begin{equation}
%  \beta_\Phi(E)
%  =
%  \frac{ 1 } { k_B \, T_\Phi(E) }
%  =
%  \frac{ \Omega(E) } { \Phi(E) }
%  =
%  \frac{ N_f } { 2 \langle K \rangle_E }
%\end{equation}
%
%\begin{equation}
%  \beta'_\Phi(E)
%  =
%  \beta_\Phi^2(E)
%  \left(
%    \left\langle \frac 1 K \right\rangle_E
%    \langle K \rangle_E
%    -1
%  \right)
%  -
%  \beta_\Phi(E)
%  \left\langle \frac 1 K \right\rangle_E
%  ,
%\end{equation}
%

This definition has several advantages.
%
First, the bulk entropy is an adiabatic invariant\cite{gibbs,
  hertz1910a, *hertz1910b, munster, becker, cagin1988, rugh2001, adib2002, dunkel2014}
  that maintains constant
  under a slow parameter change,
  %in the mechanical sense.
%
which helps establish a mechanic model
for the first law of thermodynamics.
%
Second, it is associated with a natural equipartition theorem\cite{
  khinchin, dunkel2014}
  that generalizes Eq. \eqref{eq:Tbulk}:
$\left\langle x_i \, \partial H/\partial x_j \right\rangle = \delta_{ij} \, T_G(E)$,
which makes comparison to results from the canonical ensemble easier.

\note{
  To show that
  the integrated density of states
  is an adiabatic invariant,
  we assume that
  the Hamiltonian $H(\mathbf x, \lambda)$
  depends on a parameter $\lambda$
  that changes slowly.
  %
  Then, the integrated density of states
  depends on both $E$ and $\lambda$,
  %
  $$
  \Phi(E, \lambda)
  =
  \int \Theta\bigl(
    E - H(\mathbf p, \mathbf q, \lambda)
  \bigr)
  \, d\mathbf p \, d\mathbf q
  .
  $$
  %
  The rate of change is
  \begin{align*}
  \frac{ d \Phi } { dt }
  &=
  \left(
    \frac{ \partial \Phi } { \partial E }
  \right)_\lambda
  \frac{ dE } { dt }
  +
  \left(
    \frac{ \partial \Phi } { \partial \lambda }
  \right)_E
  \frac{ d\lambda } { dt }
  .
  \end{align*}
  %
  Under Newton's equation,
  $d\mathbf p/dt = -\partial H/\partial \mathbf q$
  and
  $d\mathbf q/dt = \partial H/\partial \mathbf p$,
  we have
  $$
  \frac{ dE } { dt }
  =
  \frac{ \partial H } { \partial \mathbf p }
  \frac{ d \mathbf p } { d t }
  +
  \frac{ \partial H } { \partial \mathbf q }
  \frac{ d \mathbf q } { d t }
  +
  \frac{ \partial H } { \partial \lambda }
  \frac{ d \lambda } { dt }
  =
  \frac{ \partial H } { \partial \lambda }
  \frac{ d \lambda } { dt }
  .
  $$
  %
  For a slowly changed $\lambda$,
  we may time average $\partial H / \partial \lambda$
  to compute $dE/dt$,
  %
  \begin{align*}
  \frac{ d \Phi } { dt }
  &=
  \left(
    \frac{ \partial \Phi } { \partial E }
  \right)_\lambda
  \overline{
    \left(
    \frac{ \partial H } { \partial \lambda }
    \right)
  }
  \frac{ d\lambda } { dt }
  +
  \left(
    \frac{ \partial \Phi } { \partial \lambda }
  \right)_E
  \frac{ d\lambda } { dt }
  .
  \end{align*}
  %
  But we also have
  \begin{align*}
  \left(
    \frac{ \partial \Phi } { \partial E }
  \right)_\lambda
  &=
  \int
  \delta\bigl(
    E - H(\mathbf p, \mathbf q, \lambda)
  \bigr)
  \, d\mathbf p \, d\mathbf q
  =
  \Omega(E, \lambda)
  ,
  \\
  \left(
    \frac{ \partial \Phi } { \partial \lambda }
  \right)_E
  &=
  -\int
  \frac{ \partial H } { \partial \lambda }
  \delta\bigl(
    E - H(\mathbf p, \mathbf q, \lambda)
  \bigr)
  \, d\mathbf p \, d\mathbf q
  =
  -\left\langle
    \frac{ \partial H } { \partial \lambda }
  \right\rangle
  \Omega(E, \lambda)
  .
  \end{align*}
  So
  \begin{align*}
  \frac{ d \Phi } { dt }
  &=
  \left[
    \overline{
      \left(
        \frac{ \partial H } { \partial \lambda }
      \right)
    }
    -
    \left\langle
      \frac{ \partial H } { \partial \lambda }
    \right\rangle
  \right]
  \Omega(E, \lambda) \,
  \frac{ d\lambda } { dt }
  .
  \end{align*}
  %
  Under the ergodicity hypothesis,
  the trajectory and ensemble average
  are the same,
  the two terms in the parentheses cancel.
  %
  Thus, $d\Phi/dt = 0$.
  In other words,
  $\Phi(E, \lambda)$, hence $S_\Phi(E, \lambda)$,
  is an adiabatic invariant---a quantity that holds constant
  during an adiabatic process.
}

\note{
  The equipartion theorem can be shown as follows,
  %
  \begin{align*}
    \left\langle
      x_i \,
      \frac{ \partial H } { \partial x_j }
    \right\rangle
    \Omega(E)
    &=
    \left\langle
      x_i \,
      \frac{ \partial H } { \partial x_j }
      \,
      \delta\bigl(
        E - H(\mathbf x)
      \bigr)
    \right\rangle \\
    &=
    -
    \left\langle
      x_i \,
      \frac{ \partial } { \partial x_j }
      \Theta\bigl(
        E - H(\mathbf x)
      \bigr)
    \right\rangle
    \\
    &=
    \left\langle
      \frac{ \partial x_i } { \partial x_j }
      \Theta\bigl(
        E - H(\mathbf x)
      \bigr)
    \right\rangle
    =
    \delta_{i, j} \, \Phi(E)
    .
  \end{align*}
  %
  Thus,
  %
  \begin{align*}
    \left\langle
      x_i \,
      \frac{ \partial H } { \partial x_j }
    \right\rangle
    =
    \delta_{i, j} \, T_\Phi(E)
    .
  \end{align*}
}




\subsection{Derivative of the surface entropy}



In common textbook of statistical mechanics, however,
the microcanonical entropy
follows the surface or Boltzmann definition\cite{
  pearson1985, cagin1988, becker, dunkel2014, frenkel2015},
$S_\Omega(E) = k_B \ln \Omega(E)$.
%
The corresponding temperature and its derivative
can be derived as follows.
%
We start by introducing a vector field, $\mathbf u$,
in the phase space,
such that
%
\begin{equation}
  \mathbf u \cdot \nabla H = 1
  ,
  \label{eq:unormalization}
\end{equation}
%
anywhere, then
%
\begin{align}
  \Omega'(E)
  &= -\int \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= -\int (\mathbf u \cdot \nabla H) \,
           \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= -\int \mathbf u \cdot
           \nabla \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= \int
     \left( \nabla \cdot \mathbf u \right) \,
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  .
  \notag
  %\label{eq:dOmegaE}
\end{align}
%
where we have integrated by parts in the last step.
%
Thus, we have from Eq. \eqref{eq:betaE_def} that
%
\begin{equation}
  \beta_\Omega(E)
  =
  \frac{ \Omega'(E) } { \Omega(E) }
  =
  \left\langle
    \nabla \cdot \mathbf u
  \right\rangle_E
  .
  \label{eq:betaE_average}
\end{equation}
%
Although the vector field $\mathbf u$ can generally
involve both coordinates and momenta,
it is convenient to construct $\mathbf u$
entirely from the momenta
%
\begin{equation}
  \mathbf u
  =
  \frac{ \mathbf p }
       {  2 \, K }
  ,
  \label{eq:u_def}
\end{equation}
where $\mathbf p$ is the momenta vector in the phase space
with the coordinate components being zeroes,
and $K = \frac 1 2 \mathbf p \cdot \mathbf M^{-1} \cdot \mathbf p$
is the kinetic energy ($\mathbf M$ is the diagonal mass matrix).
%
This vector field satisfies Eq. \eqref{eq:unormalization}
because
$\mathbf u \cdot \nabla H
= (\mathbf p \cdot \nabla_{\mathbf p} K)/(2 \, K)
= \mathbf p \cdot (\mathbf M^{-1} \cdot \mathbf p) / (2 \, K) = 1$.
%
Using Eq. \eqref{eq:u_def} in Eq. \eqref{eq:betaE_average}
yields
%
\begin{equation}
  \beta_\Omega(E)
  =
  \left\langle
    \frac{ \frac{1}{2}N_f - 1 } { K }
  \right\rangle
  .
  \notag
  %\label{eq:betaE_invK}
\end{equation}

In a similar manner, we have
%
\begin{align*}
  \beta'_\Omega(E)
  =
  \frac
  {
    \Omega''(E)
  }
  {
    \Omega(E)
  }
  -
  \left[
    \frac
    {
      \Omega'(E)
    }
    {
      \Omega(E)
    }
  \right]^2
  ,
\end{align*}
%
and since
%
\begin{align*}
  \Omega''(E)
  &= -\int
      \left( \nabla \cdot \mathbf u \right) \,
      \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  \\
  &= \int
     \nabla \cdot \bigl( (\nabla \cdot \mathbf u) \, \mathbf u \bigr) \,
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  \\
  &= \int
     \left[
     \mathbf u \cdot \nabla (\nabla \cdot \mathbf u)
     +
     (\nabla \cdot \mathbf u)^2
     \right]
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  ,
\end{align*}
%
we have
%
\begin{align*}
  \beta'_\Omega(E)
  =
  \left\langle
     \mathbf u \cdot \nabla (\nabla \cdot \mathbf u)
  \right\rangle_E
  +
  \left\langle
    \Delta (\nabla \cdot \mathbf u)^2
  \right\rangle_E
  .
\end{align*}
%
With the vector field given by Eq. \eqref{eq:u_def},
we get
\begin{equation}
  \beta'(E)
  =
  -
  \left\langle
    \frac{ \frac{1}{2} N_f - 1 } { K^2 }
  \right\rangle
  +
  \left\langle
    \Delta
    \left(
      \frac{ \frac{1}{2} N_f - 1 } { K }
    \right)^2
  \right\rangle
  .
  \notag
  %\label{eq:dbetadE}
\end{equation}

In the above derivation,
we have, for simplicity, ignored the conservation
of total momenta\cite{shirts2006, uline2008},
which gives rise to additional constraints
satisfied in a typical MD simulation\cite{lado1981, wallace1983}.
%
Fortunately, with the particular choice given by Eq. \eqref{eq:u_def},
one can show that this effect is negligible for a system
with zero total momenta\cite{uline2008}
\big[a key observation here is
that for the total momenta $P_\nu = \sum_{i} p_{i, \nu}$,
$\nu = x,y,z$, we have
$\mathbf u \cdot \nabla P_\nu = P_\nu/(2K) = 0$\big].






\section{\label{sec:model}A model system}




%The ratio defined in Eq. \eqref{eq:gamma_def}
%depends on the potential energy.
%
Consider the model Hamiltonian of $\mathbf x = (\mathbf r, \mathbf v)$,
\begin{equation}
  H(\mathbf x)
  =
  \frac{\mathbf v^2} { 2 }
  +
  \left( \frac{\mathbf r^2} { 2 } \right)^\theta
  ,
\end{equation}
%
where $K = \frac 1 2 {\mathbf v}^2$ and
$U = \frac 1 2 {\mathbf r}^2$
are the kinetic and potential energy, respectively,
and $\theta$ is a positive free parameter.
Then
\begin{align*}
  \omega(E)
  &=
  C^2
  \int
    \delta\left( K + U^\theta - E \right) \,
    K^{\frac{ N_f } 2 - 1} \, dK \, U^{\frac{ N_f } 2 - 1} \, dU
  \\
  &=
  \frac{ C^2 } { \theta }
  \int
  K^{\frac{ N_f } 2 - 1} \, (E - K)^{\frac{ N_f }{ 2 \, \theta } - 1}
    \, dK
  \\
  &=
  \frac{ C^2 }{ \theta } \,
  B\left( \frac{ N_f } {2 \, \theta}, \frac{ N_f } 2 \right)
  E^{ \frac{ N_f }{2 \, \theta} + \frac{N_f}{2} - 1 }
  ,
\end{align*}
where
%
$C = 2 \, \pi^{N_f/2} / \Gamma\left( N_f / 2 \right)$,
and
$B(a, b) = \Gamma(a) \, \Gamma(b) / \Gamma(a+b)$
is the beta function.
%
Then, we have
\begin{align*}
\beta_\omega(E)
&=
\left(
  \frac{ N_f } { 2 \, \theta } + \frac{ N_f } 2 - 1
\right)
E^{-1}
,
\\
\beta'_\omega(E)
&=
-
\left(
  \frac{ N_f } { 2 \, \theta } + \frac{ N_f } 2 - 1
\right)
E^{-2}
\\
\left\langle
  \frac{
    N_f - 2
  }
  {
    2 \, K^2
  }
\right\rangle
&=
  E^{-2}
\left.
  B\left( \frac{ N_f } { 2  \, \theta } - 2, \frac{ N_f } { 2 } \right)
\middle/
  B\left( \frac{ N_f } { 2  \, \theta }, \frac{ N_f } { 2 } \right)
\right.
\\
&=
\frac{ \frac{ N_f } 2 + \frac{ N_f }{2 \, \theta} - 1 }
     { E^2 }
\frac{ \frac{ N_f } 2 + \frac{ N_f }{2 \, \theta} - 2 }
     { \frac{ N_f } 2 - 2 }
.
\end{align*}
This means the ratio defined in
\begin{equation}
  \gamma
  \equiv
  \frac
  {
    -\beta'(E^*)
  }
  {
    \overline{
      \left( \frac 1 2 N_f - 1  \right) / K_t^2
    }
  }
  ,
  %\notag
  \label{eq:gamma_def}
\end{equation}
satisfies
$$
\gamma
=
\frac
{
  \frac{ N_f } 2 - 2
}
{
  \frac{ N_f } 2 + \frac{N_f}{2 \, \theta} - 2
}
,
$$
which lies between $0$ and $1$.


For the integrated density of states, we have
$$
\Omega(E)
  =
  \frac{ C^2 }{ \theta } \,
  B\left( \frac{ N_f } {2 \, \theta}, \frac{ N_f } 2 \right)
  \frac{ E^{ \frac{ N_f }{2 \, \theta} + \frac{N_f}{2}  } }
  { \frac{ N_f } { 2 \, \theta } + \frac{ N_f } { 2 } }
$$
and
$$
k_B \, T_\Omega(E)
=
\frac{ E }
  { \frac{ N_f } { 2 \, \theta } + \frac{ N_f } { 2 } }
,
$$
which is indeed less than the surface counterpart $1/\beta_\omega(E)$.



\section{\label{sec:error}
  Alternative derivation of
  the optimal scaling magnitude
  %Eq. \eqref{eq:dE_opt}
}

\newcommand{\tmax}{\mathcal T}

Here we present an alternative derivation of Eq. \eqref{eq:dE_opt},
which shows that if in each step, we can form an independent correction
to the current estimate of the total energy,
then optimally,
this correction should be used
with the weight of $1/t$.

Let $E_t$ be the total energy at step $t$,
then the aim of velocity scaling is to converge $E_t$
to the desired value of $E^*$.
%
We assume that we can form an independent estimate of $E^*$
in step $t$, $\mathcal E_t$,
which means, equivalently, an independent correction
to the current total energy,
$\Delta \mathcal E_t = \mathcal E_t - E_{t-1}$.
%
The correction can be used in velocity scaling
to guide the amount of energy change.
%
If the independent correction were exact,
then it would suffice to perform
only one operation of velocity scaling,
with the energy change
$\delta E_t = E_t - E_{t-1}$
being identical to the correction.
%
But since
the independent correction from an MD simulation
always contains random error,
velocity scaling must be done continually.
%
We can, however, minimize the error
of the total energy, $E_T$,
at the simulation end, $t = \tmax$,
by accepting the correction with some
optimal time-dependent magnitude, $\alpha(t)$,
as
%
\begin{equation}
  \delta E_t
  =
  \alpha(t) \, \Delta \mathcal E_t
  =
  \alpha(t) \, \left( \mathcal E_t - E_{t - 1} \right)
  .
  \label{eq:Eupdate}
\end{equation}
%
%Our aim is to minimize
%the expected error of the total energy, $E_T$,
%at the end of the simulation, $t = \tmax$.

To find the optimal $\alpha(t)$,
we will switch to the continuous-time framework.
%
We define the error of the total energy at step $t$ as
$x(t) = E_{t-1} - E^*$,
then $\dot x(t) \approx \delta E_t$,
and Eq. \eqref{eq:Eupdate} can be approximated as
%
\begin{equation}
  \dot x(t)
  =
  -\alpha(t) \, x(t) + \alpha(t) \, \xi(t)
  ,
  \label{eq:x_diffeq}
\end{equation}
%
where $\xi(t) \equiv \mathcal E_t - E^*$
denotes the random fluctuation of the independent estimate at step $t$.
%
The solution of Eq. \eqref{eq:x_diffeq} is
%
\begin{equation}
  x(t)
  =
  x(1) \, e^{ -A(t) }
  +
  \int_1^t \dot u_t(\tau) \, \xi(\tau) \, d\tau
  ,
  %\label{eq:x_sol}
  \notag
\end{equation}
%
where
\begin{align*}
  A(t)
  &=
  \int_1^t \alpha(s) \, ds
  ,
\\
  u_t(\tau)
  &= \exp\left(
    -\int_\tau^t \alpha(s) \, ds
  \right)
  .
  \notag
\end{align*}
%
Thus, the average error at the end of simulation $t = \tmax$
is given by
%
\begin{align}
  \left\langle
    x(\tmax)
  \right\rangle
  &=
  \left\langle
    x(1)
  \right\rangle
  \, e^{ -A(t) }
  ,
  \label{eq:abserr}
\end{align}
%
and the square error
can be measured as
%
\begin{align}
  \left\langle
    x^2(\tmax)
  \right\rangle
  &=
  \left\langle
    x^2(1)
  \right\rangle
  \, e^{ -2 \, A(t) }
  \notag \\
  &+
  \int_1^{\tmax}
  \int_1^{\tmax}
  \dot u_{\tmax}(t) \, \dot u_{\tmax}(t') \,
  \left\langle
    \xi(t) \, \xi(t')
  \right\rangle
  \, dt \, dt'
  .
  \label{eq:err_functional}
\end{align}
%
Below we will find the optimal functional form of $\alpha(t)$
that minimizes the square error.



%\subsection{\label{sec:whitenoise}
%White-noise approximation
%}

For a very long simulation,
we may approximate $\xi(t)$ as a white noise,
such that
\begin{equation}
  \left\langle \xi(t) \, \xi(t') \right\rangle
  = \Gamma \, \delta(t - t').
  \notag
  %\label{eq:noise_corr}
\end{equation}
%
Note that this is justifiable only
if the simulation length, $\tmax$, is much longer than
the autocorrelation time of $\xi(t)$,
as shown in Fig. \ref{fig:errz}.
%
Under this assumption,
Eq. \eqref{eq:err_functional} is simplified as
%
\begin{equation}
  \left\langle
    x^2(\tmax)
  \right\rangle
  =
  \left\langle
    x^2(1)
  \right\rangle
  \, e^{ -2 \, A(t) }
  +
  \Gamma
  \int_1^{\tmax}
    \dot u_{\tmax}^2(t) \, dt
  .
  \label{eq:err_functional_wn}
\end{equation}
%
This expression is a functional of $\alpha(t)$,
or equivalently, a functional of $u_{\tmax}(t)$.
%
To minimize it under a fixed value of $A(\tmax)$,
which implies fixed values of $u_{\tmax}(1)$ and $u_{t_m}(t_m)$,
we get, from the Euler-Lagrange equation,
$$
\dot u_{\tmax}(t) = \mathrm{const},
$$
which leads to the solution,
$u_{\tmax}(t) = (t + t_0) / (t_m + t_0)$,
with $c$ and $t_0$ being two constants.
%
As a result, we get the optimal scaling magnitude
%
\begin{equation}
  \alpha(t) = \frac{ 1 } { t + t_0 }
  ,
  \notag
  %\label{eq:alpha_opt}
\end{equation}
%
and the error is
%
$$%\begin{equation}
  \left\langle
    x^2(\tmax)
  \right\rangle
  =
  \frac{
    \left\langle x^2(1) \right\rangle
    \, (1 + t_0)^2
    + \Gamma \, (\tmax - 1)
  }
  {
    (\tmax + t_0)^2
  }
  ,
$$%\end{equation}

We can further determine the optimal value of $t_0$ as
$$
t_0 = \frac{ \Gamma } { \left\langle x^2(1) \right\rangle } - 1,
$$
%
with the minimal error being $\Gamma / (\tmax + t_0)$.
%
However, for a long simulation, the influence of $t_0$
is small, and for convenience we may approximately use
$$
\alpha(t) \approx \frac 1 t,
$$
which recovers Eq. \eqref{eq:dE_opt}.

We can further show that Eq. \eqref{eq:dE_mod}
%corresponds to a modified schedule
%$\alpha(t) = z/t$,
results in an expected error of
\begin{equation}
  \left\langle
  x^2(\tmax)
  \right\rangle
  =
  \frac{ \langle x^2(1) \rangle } { \tmax^{2z} }
  +
  \frac{ \Gamma \, z^2 } { 2 \, z - 1 }
  \frac{
    \tmax^{2 \, z - 1} - 1
  }
  {
    \tmax^{2 \, z}
  }
  ,
  \notag
  %\label{eq:err_zovert}
\end{equation}
which is minimal at $z = 1$ for a long simulation.

\note{
  To determine the value of $\Gamma$ of an unknown system,
  we may conduct a simulation
  with the scaling magnitude of velocity scaling
  being a constant, $\alpha(t) = \alpha_0$.
  %
  From Eq. \eqref{eq:err_functional_wn}, we find that
  the error
  $\left\langle
    x^2(\tmax)
  \right\rangle
  =
  \Gamma \, \alpha_0 / 2$
  is independent of time $\tmax$.
  Thus, we can compute $\Gamma$ from the trajectory average
  after equilibration,
  \begin{equation}
  \Gamma
  =
  \frac{ 2 } { \alpha_0 } \,
  \overline{
    \bigl(
      E - \overline E
    \bigr)^2
  }
  .
  \notag
  %\label{eq:Gamma_alpha0}
  \end{equation}
}




%\bibliographystyle{abbrv}
\bibliography{simul}
\end{document}
