%\documentclass[12pt]{article}
\documentclass[reprint]{revtex4-1}
\usepackage{amsmath}
\begin{document}



\title{Adaptive velocity rescaling for an asymptotic
microcanonical ensemble}
\author{}
%\date{\vspace{-7ex}}
\maketitle



Velocity scaling is a temperature control scheme
for molecular dynamics (MD) simulations.
%
In a common implementation,
the velocity is regularly scaled such that the average
kinetic energy of the past few steps, $\bar K$,
can match the expected value,
$K^* = \frac{1}{2} N_f \, k_B \, T$,
where $N_f$, $k_B$, and $T$ are the number of degrees of freedom,
the Boltzmann constant, and temperature, respectively.
%
A somewhat gentler modification, the Berendsen thermostat\cite{berendsen1984},
modifies the scaling scheme, such that
the kinetic energy approaches the target value at a rate of
$\left( K^* - \bar K \right)/\tau$,
with $\tau^{-1}$ being the rate of damping.

The above schemes, albeit effective,
do not result in exact sampling for the microcanonical ensemble,
because they cause the total energy fluctuate continually.
%
Thus, they are often regarded as expedient protocols
suitable only for parameter tuning in preliminary runs,
but not for rigorous MD simulations in the microcanonical ensemble.
%
However, the energy fluctuation of velocity scaling
can be used to design thermostat algorithms,
such as the Nos\'e-Hoover\cite{nose1984, nose1984mp, hoover1985, martyna1992}
and Langevin-style velocity scaling\cite{bussi2007} thermostats,
for the canonical ensemble, in which the total energy
is intrinsically fluctuating.
%
Nonetheless, the thermostat algorithms may introduce
somewhat artificial time scales that are foreign to the natural time evolution.

Here we present a modified velocity scaling scheme
for an asymptotic microcanonical ensemble,
in which
the temperature gradually approaches the desired value with increasing precision.
%
Our modification is inspired by the $1/t$ scheme\cite{
  belardinelli2007, belardinelli2007jcp, belardinelli2008,
  zhou2005, zhou2008, morozov2007}
for improving the convergence of the Wang-Landau algorithm\cite{
  wang2001, wang2001pre}.
The WL algorithm,
which falls in a larger class of free energy methods
called metadynamics\cite{
  laio2002, laio2008, marsili2006},
has parameters for the entropy that need to be
determined adaptively,
and the $1/t$ scheme determines the optimal updating magnitude
for the convergence of the process.
%
Similarly, we may compare the total energy here
to a parameter in the WL algorithm,
and argue that the optimal magnitude
of velocity scaling should be inversely proportional
to the number of steps.
%
Besides, we adopt a slightly different definition of temperature\cite{rugh1997}
from twice the kinetic energy per degree of freedom
to make the algorithm more suitable for the microcanonical ensemble.



\section{Methods}



\subsection{Background in Statistical Mechanics}



Our aim is to obtain a temperature control scheme
for a plain MD simulation in the microcanonical ensemble.
%
This must be achieved by adjusting the total energy, $E$,
which is conserved by Newton's equation of motion.


An MD trajectory generated from
Newton's equation of motion
can be thought as a realization
of a microcanonical ensemble,
or a collection of configurations,
with the same total energy.
%
Below we shall denote a configuration in the trajectory
by a vector $\mathbf x$,
specifying the coordinates and velocities.
%
Under the ergodicity hypothesis,
Newton's equation will populate configurations
that spread evenly over
the isoenergetic hypersurface in the phase space,
defined by
$H(\mathbf x) = E$,
where $H(\mathbf x)$ is the Hamiltonian function.
%
The area of the hypersurface gives the density of states,
%
\begin{equation}
  \Omega(E)
  =
  \int
    \delta\bigl( H(\mathbf x) - E \bigr)
    \, d\mathbf x
  ,
  \label{eq:omegaE}
\end{equation}
%
and the inverse temperature, $\beta = 1/(k_B T)$
is the logarithmic derivative
%
\begin{equation}
  \beta(E)
  =
  \frac{ d }{ dE }
  \ln \Omega(E)
  =
  \frac{ \Omega'(E) }
       { \Omega(E)  }
  ,
  \label{eq:betaE_def}
\end{equation}
%
and, therefore, a function of $E$.
%

By velocity scaling, we want to
find a value of the total energy, $E^*$,
such that the microcanonical temperature
is equal to the target value, $\beta^*$,
i.e., the solution of
%
\begin{equation}
  \beta(E^*)
  =
  \beta^*
  .
  \label{eq:beta_star}
\end{equation}
%
Unfortunately, the function $\beta(E)$
is usually unknown in advance
and must be learned gradually from simulation.
%
So is the solution of Eq. \eqref{eq:beta_star}.
%
The adaptive averaging scheme offers a way
to gradually improve the knowledge of temperature function,
and hence the precision of velocity scaling.



\subsection{Adaptive averaging}



Suppose that in each MD step, $t$,
we may form an independent estimate of $E^*$,
$\mathcal E_t$,
then a more reliable estimate of $E^*$
can be obtained from averaging
all previous independent estimates,
%
\begin{equation}
  \bar{\mathcal E}_t
  =
  \sum_{\tau = 1}^t
    \mathcal E_\tau
  .
  \label{eq:Epsave}
\end{equation}
%
This is a runtime average,
and its precision improves over the simulation course.


The modified velocity scaling based on adaptive averaging
differs from the traditional version
in that it uses $\bar{\mathcal E}_t$
instead of $\mathcal E_t$
to guide the amount of energy change in each step.
%
That is, we want
the total energy of the system
at the end of each step
to be given by
the runtime average, $\bar{\mathcal E}_t$.
%
This condition specifies the amount of energy increment
by velocity scaling in step $t$
%
\begin{equation}
  \delta E_t
  =
  \bar{\mathcal E}_t - \bar{\mathcal E}_{t - 1}
  .
  \label{eq:dE_adaptive}
\end{equation}

As the runtime average from Eq. \eqref{eq:Epsave}
stabilizes, the magnitude of velocity scaling
naturally decreases.
%
This can be seen by rewriting Eq. \eqref{eq:Epsave}
as a recurrence relation
%
\begin{align}
  \bar{\mathcal E}_t
  &=
  \frac{1}{t}
  \left[
    (t - 1) \, \bar{\mathcal E}_{t - 1}
    + \mathcal E_t
  \right]
  \notag \\
  &=
  \bar{\mathcal E}_{t - 1}
  +
  \frac{
    \mathcal E_t - \bar{\mathcal E}_{t - 1}
  }
  {
    t
  }
  .
\label{eq:Epsave_recur}
\end{align}
%
Here we may understand $\mathcal E_t - \bar{\mathcal E}_{t - 1}$
as the independent correction
to the current average $\bar{\mathcal E}_{t - 1}$
from the configuration at step $t$,
and Eq. \eqref{eq:Epsave_recur} shows that
the correction is absorbed in the runtime average
with a weight of $1/t$.
%
From Eqs. \eqref{eq:dE_adaptive} and \eqref{eq:Epsave_recur},
we obtain the optimal amount of energy change
\begin{align}
  \delta E_t
  =
  \frac{ 1 } { t }
  \cdot
  \left( \mathcal E_t - \bar{\mathcal E}_{t - 1} \right)
  .
  \label{eq:dE_opt}
\end{align}
%
This formula gives the ideal amount of energy change for velocity rescaling
in every MD step.
%
Equation \eqref{eq:dE_opt} is analogous to the $1/t$, prescription\cite{
  belardinelli2007, belardinelli2007jcp, belardinelli2008,
  zhou2005, zhou2008, morozov2007}
of controlling the updating magnitude
for the Wang-Landau algorithm\cite{wang2001, wang2001pre},
and the above derivation results from its close connection\cite{
  marsili2006, barducci2008}
to the adaptive biasing force method\cite{darve2001, darve2008}.
%
In Appendix \ref{sec:error}, we shall give an alternative derivation of the result.



\subsection{Linearization}



To form an independent estimate of
$\mathcal E_t$ of $E^*$,
%
we shall form
an independent estimate of temperature, $\beta_t$
(detailed in the next subsection)
and solve Eq. \eqref{eq:beta_star}.

Linearizing Eq. \eqref{eq:beta_star} around the solution
yields
%
\begin{equation*}
\beta^*
\approx
\beta( E )
+
\beta'( E^* ) \, ( E^* - E )
,
%\label{eq:beta_linear}
\end{equation*}
%
or
%
\begin{equation}
E^*
\approx
E
+
\frac{ \beta^* - \beta(E) }
     { \beta'(E^*) }
.
\label{eq:E_inversion}
\end{equation}
%
In our adaptive velocity scaling scheme,
we require the total energy at the beginning of every step
(before scaling) to stay at the average $\mathcal E_{t - 1}$.
%
Thus, we may use $\mathcal E_{t - 1}$ for $E$
and $\beta_t$ for $\beta(E)$
in Eq. \eqref{eq:E_inversion},
and $E^*$ would be the independent estimate
$\mathcal E_{t}$ at step $t$:
%
%
\begin{equation}
\mathcal E_t
=
\bar{\mathcal E}_{t - 1}
+
\frac{ \beta^* - \beta_t }
     { \beta'(E^*) }
.
\label{eq:Eps}
\end{equation}
%
Thus, the optimal energy increment,
from Eqs. \eqref{eq:dE_opt} and \eqref{eq:Eps},
is
%
\begin{equation}
\delta E_t
=
\frac{ 1 } { t }
\cdot
\frac{ \beta^* - \beta_t }
     { \beta'(E^*) }
.
\label{eq:dE_beta}
\end{equation}
%



\subsection{Microcanonical temperature}


Next, we shall specify formulas for $\beta_t$
and $\beta'(E)$ required in Eq. \eqref{eq:dE_beta}.
%
It can be shown that both the microcanonical temperature, $\beta(E)$,
and $\beta'(E)$ can be evaluated as
averages in the microcanonical ensemble\cite{rugh1997}:
%
\begin{equation}
  \beta(E)
  =
  \left\langle
    \frac{ N_f - 2 }
         { 2 \, K }
  \right\rangle_E
  ,
  \label{eq:betaE_invK}
\end{equation}
%
and
%
\begin{equation}
  \beta'(E)
  =
  - \left\langle
      \frac{ N_f - 2 }
           { 2 \, K^2 }
    \right\rangle_E
  + \left\langle
      \Delta\left(
        \frac{ N_f - 2 }
             { 2 \, K }
      \right)^2
    \right\rangle_E
  ,
  \label{eq:dbetadE}
\end{equation}
%
where
$\langle \Delta A^2 \rangle = \langle A^2 \rangle - \langle A \rangle^2$
means the variance of quantity $A$.
%
%We will use Eq. \eqref{eq:dbetadE} to gauge
%the magnitude of adjusting the velocity-rescaling factor.
Note that the usual definition of temperature from twice the kinetic energy
per degree of freedom is, however,
derived from the canonical ensemble and
inexact for the microcanonical ensemble.



Now if we use Eq. \eqref{eq:betaE_invK} and
the one-step estimate for the inverse temperature
$\beta_t$,
then the optimal energy increment is given by
Eq. \eqref{eq:dE_beta}:
%
\begin{equation}
\delta E_t
=
\frac{ 1 } { t } \cdot
\frac{ 1 } { \beta'(E^*) }
\left(
 \beta^* -
 \frac{ N_f - 2  }
      { 2 \, K_t }
\right)
,
\label{eq:dE_final}
\end{equation}
%
where
$K_t$ is the kinetic energy before velocity scaling at step $t$,
and $\beta'(E^*)$ can be approximated
as
%
\begin{align}
  \beta'(E^*)
  \approx
  - \overline{
    \left(
      \frac{ N_f - 2 }
           { 2 \, K_t^2 }
    \right)
    }
    +
    \operatorname{Var}
    \left(
        \frac{ N_f - 2 }
             { 2 \, K_t }
    \right)
  ,
  \label{eq:dbeta}
\end{align}
%
where
$\overline A$ means the trajectory average of $A$,
and
$\operatorname{Var}(A) = \overline{ A^2 } - {\overline A}^2$
is the corresponding variance.
%


\subsection{Practical issues}


Ideally, we should have $\beta'(E) \le 0$
as the temperature $T$ increases the total energy $E$.
%
This, however, may not be true in practice
because of the numerical error of the second variance term on the right-hand side
of Eq. \eqref{eq:dbeta}.
%
%Further, since the variance is nonnegative, we have
%%
%\begin{equation*}
%  - \overline{
%    \left(
%      \frac{ N_f - 2 }
%           { 2 \, K_t^2 }
%    \right)
%    }
%  \le
%  \beta'(E^*)
%  \le
%  0,
%\end{equation*}
%
A useful approximation that avoids the variance is the following
%
\begin{equation}
  \beta'(E^*)
  =
  -\gamma \, \overline{
    \left(
      \frac{ N_f - 2 }
           { 2 \, K_t^2 }
    \right)
    }
  ,
  \label{eq:dbeta_approx}
\end{equation}
%
where we have defined the ratio $\gamma$ as
%
\begin{equation}
  \gamma
  \equiv
  \frac
  {
    -\beta'(E^*)
  }
  {
    \overline{
      \left( \frac 1 2 N_f - 1  \right) / K_t^2
    }
  }
  .
  \label{eq:gamma_def}
\end{equation}
%
For for a physical system, we expect $0 \le \gamma \le 1$.


We may use Eq. \eqref{eq:dbeta_approx} in two ways.
%
First, as a safety measure in an early stage of simulation,
we may propose a minimal value of $\gamma$, say $0.1$,
and when the value from Eq. \eqref{eq:dbeta}
exceeds that from Eq. \eqref{eq:dbeta_approx},
we will use the latter instead of the former.

Second, from
Eqs. \eqref{eq:dbeta_approx} and \eqref{eq:dbeta_approx2},
we have
%
\begin{equation}
  \beta'(E^*)
  =
  -\frac{ \gamma \, {\beta^*}^2 }
  { \frac{1}{2} N_f + \gamma - 2 }
  .
  \label{eq:dbeta_approx2}
\end{equation}
%
We can determine some heuristic value of $\gamma$
for certain type of systems,
then use Eq. \eqref{eq:dbeta_approx2}
instead of Eq. \eqref{eq:dbeta}.
%
For example,
for an explicit solvent simulation of TIP3P water\cite{jorgensen1983}
at 300K, we find that the optimal $\gamma$
to be around $0.33$.



\section{Results}


[
Compare the fluctuation of energy caused by velocity scaling to
that caused by numerical integration of the equation of motion.

%Relation to the $1/t$ recipe for Wang-Landau dynamics.

Show that the optimal convergence to the right total energy.
]

As shown in Fig. 1,
the adaptive velocity scaling yielded a narrower distribution
of the kinetic energy than the canonical counterpart
with the same center.
%
The canonical ensemble, as a superposition
of microcanonical ones,
tends to broaden distributions.
%
More interestingly,
Fig. 2 shows that the adaptive velocity scaling
yields shorter correlation for the kinetic energy.

Use
\begin{equation}
\delta E_t
=
\frac{ 1 } { t } \cdot
\frac{ \eta } { \beta'(E^*) }
\left(
 \beta^* -
 \frac{ N_f - 2 }
 { 2 \, K_t }
\right)
,
\label{eq:dE_eta}
\end{equation}
%
instead of Eq. \eqref{eq:dE_final}.


\section{Discussion}


%As time $t$ increases, the scaling tends to $1$,
%and this gradually decreased magnitude of velocity change
%allows the ensemble to asymptotically approach a
%microcanonical one with conserved total energy.
%%
%Thus, the approach differs from the Langevin-style
%velocity scaling and Nos\'e-Hoover thermostats,
%which target a canonical ensemble.


In practice, velocity scaling is often implemented every few,
say $m$ ($m > 1$), MD steps.
%
Then $t$ should be understood as the number of
such velocity-scaling operations so far instead of
the number of MD steps, and $K_t$ in Eq. \eqref{eq:dE_final} is understood
as the block average of the kinetic energy of the $m$ steps
before the scaling operation.


\appendix


\section{\label{sec:error}
  Alternative derivation of Eq. \eqref{eq:dE_opt}
}


Here, we present an alternative derivation of Eq. \eqref{eq:dE_opt},
which shows that if in each step, we can form an independent correction
to the total energy, such a correction should be optimally used
in velocity scaling with an ever-decreasing weight of $1/t$.

Let $E_t$ be the total energy at step $t$,
then the aim of velocity scaling is to converge $E_t$
to the desired value of $E^*$.
%
We assume that we can form an independent estimate of $E^*$
in step $t$, $\mathcal E_t$,
which means, equivalently, an independent correction
to the current total energy, $\mathcal E_t - E_{t-1}$.
%
The correction can be used to guide the amount of
energy change, $\Delta E_t = E_t - E_{t-1}$,
for velocity rescaling.
%
If the independent correction were exact,
then velocity rescaling only needs to be done once,
with the change $\Delta E_t$ to be identical to the correction.
%
But since the independent correction contains random error,
velocity rescaling must be done continually,
and as we shall see,
if we accept the correction with some
optimal time-dependent magnitude, $\alpha(t)$, as
%
\begin{equation}
  \Delta E_t = \alpha(t) \, \left( \mathcal E_t - E_{t - 1} \right)
  ,
  \label{eq:Eupdate}
\end{equation}
%
the expected error of the total energy, $E_T$,
at the end of the simulation, $t = T$, can be minimized.

To find the optimal $\alpha(t)$,
it is more convenient to use the continuous-time framework.
%
We introduce the error of the total energy at step $t$ as
$x(t) = E_{t-1} - E^*$,
then $\dot x(t) \approx \Delta E_t$,
and Eq. \eqref{eq:Eupdate} can be approximated as
%
\begin{equation}
  \dot x(t)
  =
  -\alpha(t) \, x(t) + \alpha(t) \, \xi(t)
  ,
  \label{eq:x_diffeq}
\end{equation}
%
where $\xi(t) \equiv \mathcal E_t - E^*$
denotes the random fluctuation of the independent estimate at step $t$.
For simplicity, we will further approximate $\xi(t)$ as a white noise,
such that $\langle \xi(t) \, \xi(t') \rangle = \Gamma \, \delta(t - t')$.
%
The solution of Eq. \eqref{eq:x_diffeq} is
%
\begin{equation}
  x(t)
  =
  x(0) \, e^{ -\int_0^t \alpha(\tau) \, d\tau }
  +
  \int_0^t \dot u_t(\tau) \, \xi(\tau) \, d\tau
  ,
  \label{eq:x_sol}
\end{equation}
%
where
\begin{equation}
  u_t(\tau) = \exp\left(
    -\int_\tau^t \alpha(s) \, ds
  \right)
  .
\end{equation}
%
Thus, the error at the end of simulation $t = T$,
can be measured as expectation of $x^2(T)$,
%
\begin{equation}
  \left\langle
    x^2(T)
  \right\rangle
  =
  \left\langle
    x^2(0)
  \right\rangle
  \, e^{ -2 \, A(t) }
  +
  \Gamma
  \int_0^T
    \dot u_T^2(t) \, dt
  ,
  \label{eq:error_functional}
\end{equation}
where
$A(t) = \int_0^t \alpha(t) \, dt$.
%
%the total energy change induced by velocity scaling
%is
%
This expression is a functional of $\alpha(t)$,
or equivalently, a functional of $u_T(t)$.
%
To minimize it under a fixed value of $A(T)$,
which implies fixed values of $u_T(0)$ and $u_T(T)$,
we get the condition
$$
\dot u_T(t) = \mathrm{const},
$$
which leads to the solution of
$u_T(t) = (t + t_0) / (T + t_0)$,
with $c$ and $t_0$ being two constants.
%
As a result, we get the optimal schedule
%
\begin{equation}
  \alpha(t) = \frac{ 1 } { t + t_0 }
  ,
  \label{eq:alpha_opt}
\end{equation}
%
and the error under this schedule is
%
\begin{equation}
  \left\langle
  x^2(T)
  \right\rangle
  =
  \frac{
    \langle x^2(0) \rangle \, t_0^2
    + \Gamma \, T
  }
  {
    (T + t_0)^2
  }
  ,
\end{equation}
%
For a long simulation, the influence of $t_0$
is relatively small,
and we may approximately use
$$
\alpha(t) \approx \frac 1 t,
$$
This result recovers Eq. \eqref{eq:dE_opt}.



\section{Derivation of the microcanonical temperature}


Here we derive Eqs. \eqref{eq:betaE_invK} and \eqref{eq:dbetadE}.
We start by defining a vector field, $\mathbf u$,
such that
%
\begin{equation}
  \mathbf u \cdot \nabla H = 1
  ,
  \label{eq:unormalization}
\end{equation}
%
anywhere, then
%
\begin{align}
  \Omega'(E)
  &= -\int \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= -\int (\mathbf u \cdot \nabla H) \,
           \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= -\int \mathbf u \cdot
           \nabla \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= \int \nabla \cdot \mathbf u \,
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  .
  \notag
  %\label{eq:dOmegaE}
\end{align}
%
where we have integrated by parts in the last step.
%
Thus, we have from Eq. \eqref{eq:betaE_def} that
%
\begin{equation}
  \beta(E)
  =
  \left\langle
    \nabla \cdot \mathbf u
  \right\rangle_E
  .
  \label{eq:betaE_average}
\end{equation}
%
Although the vector field $\mathbf u$ may involve both coordinates and velocities,
a convenient choice of $\mathbf u$ that satisfies
Eq. \eqref{eq:unormalization}
is one derived entirely from the velocities
%
\begin{equation}
  \mathbf u
  =
  \frac{ \mathbf v }
       {  2 \, K }
  ,
  \label{eq:u_def}
\end{equation}
where $\mathbf v$ is the velocity vector in the phase space
with the components for coordinates being zeroes,
and $K = \frac 1 2 \mathbf v \cdot \mathbf M \cdot \mathbf v$
is the kinetic energy.
%
Equation \eqref{eq:unormalization} is satisfied
because
$\mathbf u \cdot \nabla H
= (\mathbf v \cdot \nabla K)/(2 \, K)
= \mathbf v \cdot (\mathbf M \cdot \mathbf v) / (2 \, K) = 1$.
%
Using Eq. \eqref{eq:u_def} in Eq. \eqref{eq:betaE_average} yields Eq. \eqref{eq:betaE_invK}

In a similar manner, we have
%
\begin{align*}
  \frac
  {
    d \beta(E)
  }
  {
    d E
  }
  =
  \frac
  {
    \Omega''(E)
  }
  {
    \Omega(E)
  }
  -
  \left[
    \frac
    {
      \Omega'(E)
    }
    {
      \Omega(E)
    }
  \right]^2
  ,
\end{align*}
%
and
%
\begin{align*}
  \Omega''(E)
  &= -\int \nabla \cdot \mathbf u \,
     \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  \\
  &= \int
     \nabla \cdot \bigl( (\nabla \cdot \mathbf u) \, \mathbf u \bigr) \,
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  \\
  &= \int
     \left[
     \mathbf u \cdot \nabla (\nabla \cdot \mathbf u)
     +
     (\nabla \cdot \mathbf u)^2
     \right]
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  .
\end{align*}
%
This means
%
\begin{align*}
  \beta'(E)
  =
  \left\langle
     \mathbf u \cdot \nabla (\nabla \cdot \mathbf u)
  \right\rangle_E
  +
  \left\langle
    \Delta (\nabla \cdot \mathbf u)^2
  \right\rangle_E
  .
\end{align*}
%
With Eq. \eqref{eq:u_def},
this yields Eq. \eqref{eq:dbetadE}.



\subsection{Model system}


Consider the model Hamiltonian of $\mathbf x = (\mathbf r, \mathbf v)$,
\begin{equation}
  H(\mathbf x)
  =
  \frac{\mathbf v^2} { 2 }
  +
  \left( \frac{\mathbf r^2} { 2 } \right)^\theta
  ,
\end{equation}
%
where $K = \frac 1 2 {\mathbf v}^2$ and
$U = \frac 1 2 {\mathbf r}^2$
are the kinetic and potential energy, respectively,
and $\theta$ is a positive free parameter.
Then
\begin{align*}
  \Omega(E)
  &=
  C^2
  \int
    \delta\left( K + U^\theta - E \right) \,
    K^{\frac{ N_f } 2 - 1} \, dK \, U^{\frac{ N_f } 2 - 1} \, dU
  \\
  &=
  \frac{ C^2 } { \theta }
  \int
  K^{\frac{ N_f } 2 - 1} \, (E - K)^{\frac{ N_f }{ 2 \, \theta } - 1}
    \, dK
  \\
  &=
  \frac{ C^2 }{ \theta } \,
  B\left( \frac{ N_f } {2 \, \theta}, \frac{ N_f } 2 \right)
  E^{ \frac{ N_f }{2 \, \theta} + \frac{N_f}{2} - 1 }
  ,
\end{align*}
where
%
$C = 2 \, \pi^{N_f/2} / \Gamma\left( N_f / 2 \right)$,
and
$B(a, b) = \Gamma(a) \, \Gamma(b) / \Gamma(a+b)$
is the beta function.
%
Then, we have
\begin{align*}
\beta(E)
&=
\left(
  \frac{ N_f } { 2 \, \theta } + \frac{ N_f } 2 - 1
\right)
E^{-1}
,
\\
\beta'(E)
&=
-
\left(
  \frac{ N_f } { 2 \, \theta } + \frac{ N_f } 2 - 1
\right)
E^{-2}
\\
\left\langle
  \frac{
    N_f - 2
  }
  {
    2 \, K^2
  }
\right\rangle
&=
  E^{-2}
\left.
  B\left( \frac{ N_f } { 2  \, \theta } - 2, \frac{ N_f } { 2 } \right)
\middle/
  B\left( \frac{ N_f } { 2  \, \theta }, \frac{ N_f } { 2 } \right)
\right.
\\
&=
\frac{ \frac{ N_f } 2 + \frac{ N_f }{2 \, \theta} - 1 }
     { E^2 }
\frac{ \frac{ N_f } 2 + \frac{ N_f }{2 \, \theta} - 2 }
     { \frac{ N_f } 2 - 2 }
.
\end{align*}
This means the ratio
$$
\gamma
=
\frac
{
  \frac{ N_f } 2 - 2
}
{
  \frac{ N_f } 2 + \frac{N_f}{2 \, \theta} - 2
}
,
$$
which lies between $0$ and $1$.

%\bibliographystyle{abbrv}
\bibliography{simul}
\end{document}
