%\documentclass[12pt]{article}
\documentclass[reprint]{revtex4-1}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
\usepackage{hyperref}

\hypersetup{
  colorlinks,
  linkcolor={red!30!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{64,0,42}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}
\newcommand{\answer}[1]{{\color{DarkBlue}\footnotesize \textsc{Answer.} #1}}
\newcommand{\summary}[1]{{\color{DarkPurple}\footnotesize \textsc{Summary.} #1}}



\begin{document}



\title{Adaptive velocity scaling for an asymptotic
microcanonical ensemble}

\author{Cheng Zhang}
\author{Justin A. Drake}
\author{B. Montgomery Pettitt}
\email{bmpettitt@utmb.edu}
\affiliation{
Sealy Center for Structural Biology and Molecular Biophysics,
The University of Texas Medical Branch,
Galveston, Texas 77555-0304, USA}

%\date{\vspace{-7ex}}
\begin{abstract}
  We present a modification of the commonly-used
  temperature control scheme: velocity scaling.
  %
  Albeit its simplicity and effectiveness,
  velocity scaling creates unnatural fluctuation
  of the total energy, which distorts
  the intended sampling distribution
  from the underlying microcanonical ensemble.
  %
  We can minimize this adverse effect by
  gradually reducing the magnitude of velocity scaling.
  %
  %Using the $1/t$ prescription for improving the Wang-Landau algorithm,
  We show that the optimal reduction factor of the scaling magnitude
  is given by the general $1/t$ prescription,
  with $t$ being the number of velocity scaling operations.
  %
  In this way,
  the total energy can quickly tend to the value
  at the target temperature
  with minimal fluctuation,
  and the simulation ensemble often becomes
  indistinguishable from a microcanonical one.
  %
  TODO: extensions.
\end{abstract}

\maketitle



Velocity scaling is a temperature control scheme
for molecular dynamics (MD) simulations.
%
It is usually implemented as
regular velocity scaling operations
that correct the average
kinetic energy of the past few steps, $\bar K$,
to the expected value\cite{frenkel, haile},
%
\begin{equation}
  K^* = \frac{1}{2} N_f \, k_B \, T^*
  ,
  \label{eq:K_canon}
\end{equation}
%
where $N_f$, $k_B$, and $T^*$ are the number of degrees of freedom,
the Boltzmann constant, and the target temperature, respectively.
%
In order words,
the velocity is scaled by a factor of
$\sqrt{ K^* / \bar K }$
in each scaling operation.
%
We refer to this version as the regular velocity scaling below.
%
Velocity scaling has some variants,
among which
the Berendsen thermostat\cite{berendsen1984}
is a notable example.
%
There,
the kinetic energy is changed in each scaling operation
by
%
\begin{equation}
  \delta E = \frac{ K^* - \bar K } { \tau },
  \label{eq:Berendsen}
\end{equation}
%
with $\tau$ being a damping constant.

The above schemes, albeit effective,
do not result in exact sampling for the microcanonical ($NVE$) ensemble\cite{hermansson1988},
because they cause the total energy to fluctuate continually.
%
Thus, they are often regarded as expedient protocols
suitable only for parameter tuning in preliminary MD runs.
%but not for rigorous MD simulations in the microcanonical ensemble.
%
Nor are these schemes exact for the canonical ($NVT$) ensemble,
in which the total energy is intrinsically fluctuating.
%
Fortunately, there are several ingenious ways
of modifying the energy fluctuation caused by velocity scaling
to emulate the effect of a heat-bath,
resulting in exact thermostat algorithms for the $NVT$ ensemble,
e.g., the Nos\'e-Hoover\cite{nose1984, nose1984mp, hoover1985, martyna1992}
and Langevin-style velocity rescaling\cite{bussi2007} thermostats.
%
There,
the magnitude of velocity scaling is carefully
controlled to make
the resulting fluctuation of the total energy
compatible with the underlying canonical distribution.


The existence of exact thermostat algorithms for the $NVT$ ensemble
suggests the possibility of designing
precise temperature controls also
for the $NVE$ ensemble.
%
However, we immediately face a dilemma:
%
as the microcanonical temperature is a function of the total energy, $E$,
one must somehow change the total energy to control the temperature;
%
but doing so affects the conservation of the total energy,
and hence the accuracy of the MD simulation\cite{hermansson1988}.
%resulting in deviation from the desired microcanonical ensemble.
%
In this study, we will follow a compromised solution,
in which the magnitude of velocity scaling is gradually toned down.
%
In this way,
the temperature controller can
%get close to
zero in on
the proper value of the total energy
early in simulation,
and gradually improve its precision
in later stages.
%while its long time influence is often negligible.


%As a result of the above strategy
Accordingly,
we will propose a modified velocity scaling scheme,
which will be referred to as adaptive velocity scaling below,
aiming for an \emph{asymptotic} microcanonical ensemble,
in which
the total energy gradually tends to the value
at the target temperature.
%
The effectiveness of this modification,
in terms of the convergence of the total energy,
depends on how
the magnitude of velocity scaling, $\alpha(t)$,
varies with the number of steps, $t$.
%
%Fortunately, we may borrow the optimal scheme $\alpha(t)$
To find the optimal schedule of $\alpha(t)$,
we will borrow the recipe from the $1/t$ prescription\cite{
  belardinelli2007, *belardinelli2007jcp, *belardinelli2008,
  zhou2005, *zhou2008, *morozov2007}
for improving the convergence of
the Wang-Landau (WL) algorithm\cite{
  wang2001, *wang2001pre}.
%
The WL algorithm,
which falls in a larger class of free energy methods
of metadynamics\cite{
  laio2002, laio2008, marsili2006},
has parameters for the entropy that need to be
determined and updated adaptively,
and the $1/t$ prescription gives the optimal updating magnitude
for the parameters to reach their target values.
%
By analogy, we may compare the total energy here
to a parameter in the WL algorithm,
and speculate that the optimal magnitude
of velocity scaling would be
the inverse number of steps.
%inversely proportional to the number of steps.
%
%Such a modification could hopefully gradually decrease
We will show that this is indeed the case:
the $1/t$ modified velocity scaling can reduce
the fluctuation of the total energy
at an optimal rate.
%
%Besides the above change,
%our modification will target a % a more accurate
%microcanonical temperature\cite{rugh1997}
%based on the surface or Boltzmann entropy\cite{
%frenkel2015},
%which turns out to be different
%from the usual definition, Eq. \eqref{eq:K_canon}.
%
As we will see,
the magnitude of velocity scaling
often decreases so rapidly that
a sufficiently long simulation under adaptive velocity scaling
is almost identical to a regular MD
in conserving the total energy and producing other quantities
of the microcanonical ensemble.



\section{Methods}



\subsection{Background in statistical mechanics}



%Our adaptive velocity scaling scheme aims at
%controlling the temperature for an MD simulation.
Our goal is to design a temperature controller for a plain MD simulation.
%
Since MD follows Newton's equation of motion
that conserves the total energy, $E$,
the resulting trajectory can be described in statistical mechanics
by a microcanonical or $NVE$ ensemble,
which is a collection of configurations in the phase space
with the same total energy, $E$.
%
Unlike the more commonly-used canonical or $NVT$ ensemble,
temperature, $T$, is not a parameter of the microcanonical ensemble,
but only an observable
that is influenced by the ensemble parameter, $E$.
%
In other words,
we may write temperature
only as a function of the total energy, $T = T(E)$,
in the microcanonical ensemble.
%
Accordingly, the desired controller must adjust the total energy
to effect temperature changes.


Let us now review the temperature function, $T(E)$,
%relation between temperature, $T$,
%and total energy, $E$,
in the microcanonical ensemble.
%
We will denote a configuration in the trajectory
by a vector $\mathbf x$,
encompassing both the coordinates and momenta.
%
Under the ergodicity hypothesis,
Newton's equation will populate configurations
that spread evenly over
the isoenergetic hypersurface in the phase space,
defined by
$H(\mathbf x) = E$,
where $H(\mathbf x)$ is the Hamiltonian function.
%
Then, thermodynamic properties, such as the entropy and temperature,
of the MD trajectory
can be described by the geometry of the isoenergetic hypersurface.
%
In particular, the hypervolume enclosed by the hypersurface
%
\begin{equation}
  \Phi(E)
  =
  \int
    \Theta\bigl( E - H(\mathbf x) \bigr)
    \, d\mathbf x
  ,
  \notag
  %\label{eq:PhiE}
\end{equation}
%
defines a bulk or volume entropy\cite{
  cagin1988, haile, dunkel2014}
as $S_\Phi = \ln \Phi(E)$,
where $\Theta(y)$ is the step function,
which gives $1.0$ for a positive $y$, or $0.0$ otherwise.
%
The hypervolume is
an adiabatic invariant\cite{
  gibbs, hertz1910a, hertz1910b, munster, becker,
  cagin1988, rugh2001, adib2002, dunkel2014}
in the mechanical sense,
i.e. it would remain a constant
when a system parameter, such as volume,
changes slowly,
and, therefore, its logarithm serves as
a natural definition of the entropy.
%
The derivative of the entropy
%
\begin{equation}
  \frac{1}{ k_B \, T_\Phi(E) }
  =
  \frac{ d }{ dE }
  \ln \Phi(E)
  =
  \frac{ \Phi'(E) }
       { \Phi(E)  }
  .
  \notag
  %\label{eq:betaE_def}
\end{equation}
%
defines the corresponding temperature function, $T_\Phi(E)$,
for the microcanonical ensemble.

Alternatively, the entropy may be defined
as the logarithm of the density of states,
$\Omega(E) \equiv \Phi'(E)$,
%
\begin{equation}
  \Omega(E)
  =
  \int
    \delta\bigl( E - H(\mathbf x) \bigr)
    \, d\mathbf x
  ,
  \label{eq:DOSE}
\end{equation}
%
which gives rise to a surface microcanonical temperature
%
\begin{equation}
  \frac{1}{ k_B \, T_\Omega(E) }
  =
  \frac{ d }{ dE }
  \ln \Omega(E)
  =
  \frac{ \Omega'(E) }
       { \Omega(E)  }
  .
  \label{eq:betaE_surf_def}
\end{equation}



Either temperature definition can be converted to
an average in the microcanonical ensemble,
which is defined for quantity, $A(\mathbf x)$, as
$$
  \langle A \rangle_E
  \equiv
  \frac{ 1 } { \Omega(E) }
  \int A(\mathbf x) \,
    \delta\bigl( E - H(\mathbf x) \bigr) \, d\mathbf x
  .
$$
Under the ergodicity hypothesis,
this ensemble average is equivalent to a trajectory average
which can be readily computed from MD.
%
For example, for the bulk definition, we have
%
\begin{equation}
  k_B \, T_\Phi(E)
  =
  \frac{ 2 \left\langle K \right\rangle_E } { N_f }
  ,
  \label{eq:TE_K}
\end{equation}
%
and for the surface definition,
%
\begin{equation}
  \frac{ 1 } { k_B \, T_\Omega(E) }
  =
  \left\langle
  \frac{ \frac{1}{2} N_f - 1 } { K }
  \right\rangle_E
  .
  \notag
  %\label{eq:betaE_invK}
\end{equation}
%
Since Eq. \eqref{eq:TE_K} is compatible
with the conventional definition,
Eq. \eqref{eq:K_canon},
we will assume the bulk definition
unless specified otherwise,
i.e. $T(E)$ means $T_\Phi(E)$.
%
%


By velocity scaling, we wish to
find a value of the total energy, $E^*$,
at which the microcanonical temperature
matches the target value, $T^*$,
i.e. the solution of
%
\begin{equation}
  T(E^*)
  =
  T^*
  .
  \label{eq:T_star}
\end{equation}
%
For an unknown system,
both the temperature function, $T(E)$,
and the solution of Eq. \eqref{eq:T_star}
must be learned gradually from simulation.
%
To gradually improve
our knowledge of the temperature function,
and hence the estimate of the target energy, $E^*$,
we will resort to the adaptive averaging technique
described below.



\subsection{Adaptive averaging}



Suppose that in each MD step, $t$,
we may form an independent estimate of $E^*$,
$\mathcal E_t$,
then a more reliable estimate of $E^*$
would be the average of
all previous independent estimates,
%
\begin{equation}
  \bar{\mathcal E}_t
  =
  \frac 1 t
  \sum_{\tau = 1}^t
    \mathcal E_\tau
  .
  \label{eq:Epsave}
\end{equation}
%
Naturally, the precision of the runtime average
improves over the simulation course.



Our adaptive velocity scaling
improves over the regular velocity scaling
by using the above runtime average,
$\bar{\mathcal E}_t$,
instead of $\mathcal E_t$
to guide the amount of energy change in each step.
%
That is, we want
the total energy of the system
at the end of each step
to be given by
the runtime average, $\bar{\mathcal E}_t$.
%instead of $\mathcal E_t$.
%
This condition requires the amount of energy increment
by velocity scaling in step $t$ to be
%
\begin{equation}
  \delta E_t
  =
  \bar{\mathcal E}_t - \bar{\mathcal E}_{t - 1}
  .
  \label{eq:dE_adaptive}
\end{equation}

As the runtime average from Eq. \eqref{eq:Epsave}
stabilizes, the magnitude of velocity scaling
naturally decreases.
%
This can be seen by rewriting Eq. \eqref{eq:Epsave}
as a recurrence relation
%
\begin{align}
  \bar{\mathcal E}_t
  &=
  \frac{1}{t}
  \left[
    (t - 1) \, \bar{\mathcal E}_{t - 1}
    + \mathcal E_t
  \right]
  \notag \\
  &=
  \bar{\mathcal E}_{t - 1}
  +
  \frac{
    \mathcal E_t - \bar{\mathcal E}_{t - 1}
  }
  {
    t
  }
  .
\label{eq:Epsave_recur}
\end{align}
%
Here we may understand $\mathcal E_t - \bar{\mathcal E}_{t - 1}$
as the independent correction
to the current average $\bar{\mathcal E}_{t - 1}$
from the configuration at step $t$,
and Eq. \eqref{eq:Epsave_recur} shows that
when the correction is absorbed in the runtime average
it carries the decreasing weight of $1/t$,
as a result of the increasing sample size.
%
From Eqs. \eqref{eq:dE_adaptive} and \eqref{eq:Epsave_recur},
we obtain the optimal amount of energy change
for each MD step
\begin{align}
  \delta E_t
  =
  \frac{ 1 } { t }
  \cdot
  \left( \mathcal E_t - \bar{\mathcal E}_{t - 1} \right)
  .
  \label{eq:dE_opt}
\end{align}
%
This amount of energy change is to be realized by velocity scaling.
%
Equation \eqref{eq:dE_opt} is essentially
an application of
the $1/t$ prescription\cite{
  belardinelli2007, *belardinelli2007jcp, *belardinelli2008,
  zhou2005, *zhou2008, *morozov2007}
for the optimal control of the updating magnitude
of the WL algorithm\cite{wang2001, *wang2001pre},
and the above derivation results from its close connection\cite{
  marsili2006, barducci2008}
to the adaptive biasing force method\cite{darve2001, *darve2008}.
%



\subsection{Linearization}



Next, we need to form an independent estimate of
$\mathcal E_t$ of $E^*$.
%
For this, we will form
an independent estimate of
the instantaneous temperature, $T_t$,
and solve a linearized version of Eq. \eqref{eq:T_star}.

Linearizing Eq. \eqref{eq:T_star} around the solution
yields
%
\begin{equation*}
T^*
\approx
T( E )
+
T'( E^* ) \, ( E^* - E )
,
%\label{eq:T_linear}
\end{equation*}
%
or
%
\begin{equation}
E^*
\approx
E
+
\frac{ T^* - T(E) }
     { T'(E^*) }
.
\label{eq:E_inversion}
\end{equation}
%
In adaptive velocity scaling,
we require the total energy at the beginning of every step
(before scaling) to match the previous runtime average,
$\bar{\mathcal E}_{t - 1}$.
%
Thus, if we use $\bar{\mathcal E}_{t - 1}$ for $E$
and $T_t$ for $T(E)$
in Eq. \eqref{eq:E_inversion},
$E^*$ would be the independent estimate
$\mathcal E_{t}$ at step $t$:
%
%
\begin{equation}
\mathcal E_t
=
\bar{\mathcal E}_{t - 1}
+
\frac{ T^* - T_t }
     { T'(E^*) }
.
\label{eq:Eps}
\end{equation}
%
Thus, the optimal energy increment,
from Eqs. \eqref{eq:dE_opt} and \eqref{eq:Eps},
is
%
\begin{equation}
\delta E_t
=
\frac{ 1 } { t }
\cdot
\frac{ T^* - T_t }
     { T'(E^*) }
.
\label{eq:dE_T}
\end{equation}
%



\subsection{Microcanonical temperature}


We now give formulas for $T_t$
and $T'(E)$ required in Eq. \eqref{eq:dE_T}.
%
%It can be shown that both the microcanonical temperature, $T(E)$,
%and $T'(E)$ can be evaluated as
%averages in the microcanonical ensemble\cite{cagin1988, rugh1997}:
%
For the instantaneous temperature, $T_t$,
we can use the quantity under average in Eq. \eqref{eq:TE_K},
or $2 \, K_t/(k_B \, N_f)$,
where
$K_t$ is the kinetic energy before velocity scaling at step $t$.
%
Thus, the optimal energy increment is given by
Eq. \eqref{eq:dE_T}:
%
\begin{equation}
\delta E_t
=
\frac{ 1 } { t } \cdot
\frac{ 1 } { k_B \, T'(E^*) }
\left(
 k_B \, T^*
 -
 \frac{ 2 \, K_t } { N_f }
\right)
,
\label{eq:dE_final}
\end{equation}
%
where the derivative, $T'(E)$
can be computed as\cite{cagin1988}
%
\begin{align}
  k_B \, T'(E^*)
  %&=
  %1
  %-
  %T(E) \, \beta_\Omega(E)
  %\notag
  %\\
  &=
  1
  -
  \left(
    1 - \frac{2}{N_f}
  \right)
  \left\langle
  K
  \right\rangle_E
  \left\langle
    \frac 1 K
  \right\rangle_E
  %\\
  %&=
  %k_B \, T(E) \,
  %\left\langle K^{-1} \right\rangle_E
  %-
  %\left\langle
  %  \Delta K
  %  \Delta \left( K^{-1} \right)
  %\right\rangle
  \notag
  %\label{eq:dTdE}
  \\
  &\approx
  1
  -
  \left(
    1 - \frac{ 2 } { N_f }
  \right)
  \overline{ K }
  \,
  \overline{ \left( \frac 1 K \right) }
  .
  \label{eq:dT}
\end{align}
%
Here,
$\overline A$ means the trajectory average of $A$.
%and
%$\operatorname{Var}(A) = \overline{ A^2 } - {\overline A}^2$
%is the corresponding variance.

In practice, velocity scaling is often implemented
only every $m$ MD steps.
%
In this case,
we can replace the number of MD steps, $t$,
in Eq. \eqref{eq:dE_final} by
the number of velocity-scaling operations
(i.e. $t \to t/m$).
%
The $K_t$ in the formula can be interpreted as
the kinetic energy right before scaling
or the average of values in the past $m$ steps
[see Appendix \ref{sec:block}].




\subsection{\label{sec:gamma}
Approximations on $T'(E^*)$}


Ideally, we should have $T'(E) \ge 0$
as the temperature $T$ increases the total energy $E$.
%
This, however, may not be true in practice
because of numerical error.
%
%Further, since the variance is nonnegative, we have
%%
%\begin{equation*}
%  - \overline{
%    \left(
%      \frac{ N_f - 2 }
%           { 2 \, K_t^2 }
%    \right)
%    }
%  \le
%  \beta'(E^*)
%  \le
%  0,
%\end{equation*}
%
To avoid the variance term,
we may define a ratio
%
\begin{equation}
  \gamma
  \equiv
  \frac
  {
    d \langle K \rangle_E
  }
  {
    dE
  }
  ,
  %\notag
  \label{eq:gamma_def}
\end{equation}
%
such that
%
\begin{equation}
  k_B \, T'(E^*)
  =
  \frac{ 2 \, \gamma } { N_f }
  .
  \label{eq:dT_approx}
\end{equation}
%
For a physical system, we expect $0 \le \gamma \le 1$.
%
%The value of $\gamma$ is computed for a model system
%in Appendix \ref{sec:model}.

With the above definition, we can rewrite Eq. \eqref{eq:dE_final} as
%
\begin{equation}
  \delta E_t
  =
  \frac{ K^* - K_t } { \gamma \, t }
  .
  \notag
  %\label{eq:dE_gamma}
\end{equation}
%
Comparing this to Eq. \eqref{eq:Berendsen},
we find the adaptive velocity scaling scheme
can be considered as a modification of the Berendsen thermostat
with the damping factor, $\tau$, being replaced by $\gamma \, t$,
which increases as simulation progresses.


We can use the ratio in two ways.
%
First, we may propose a minimal value of $\gamma$, say $0.1$,
to prevent an overly small value from Eq. \eqref{eq:dT}.

Second,
we can determine an approximate heuristic value of $\gamma$
that is applicable for a class of similar systems.
%
For example,
for an explicit solvent simulation of TIP3P water\cite{jorgensen1983}
at $T = 300\mathrm K$, we find that the optimal $\gamma$
to be around $0.33$,
more generally, $\gamma \approx 0.279\ln T - 1.255$.


\subsection{Generalization}



The above adaptive velocity scaling scheme
is a special application of the $1/t$ prescription.
%
In this case we have adjusted the total energy
to achieve the target temperature.
%
More generally, we may apply this technique
to a pair of strongly correlated variables $X$ and $Y$,
to find the optimal value of a parameter, $X$,
that corresponds to a target value, $Y^*$, of an observable, $Y$.
%on the fly.
%
Then, the change of $X$ in each step is given by
%
\begin{equation}
  \delta X_t = \frac 1 t
  \cdot
  \frac{ Y^* - Y_t }
       { Y'(X^*) }
  .
  \label{eq:invt_XY}
\end{equation}
%


Below we list a few examples,
in which the derivative, $Y'(X)$,
can be computed from rigorous expressions
derived from statistical mechanics,
%
If, however, such expression is unavailable,
we can resort to a trial simulation
with a constant magnitude, $k$,
%
\begin{equation}
  \delta X_t =
  k \, (Y^* - Y_t)
  ,
  \notag
  \label{eq:Berendsen_XY}
\end{equation}
%
and then
%
\begin{equation}
  Y'(X^*)
  \approx
  k \, \tau_Y \,
  \frac{
    \mathrm{var} (Y)
  }
  {
    \mathrm{var} (X)
  }
  ,
  \label{eq:dYdX_estimate}
\end{equation}
%
where $\tau_Y$ is the integrated autocorrelation time of $Y$.
%
For example, in the velocity scaling case ($X = E, Y = K$), we have
$K'(E^*) \approx k \, \tau_K \, \mathrm{var} (K) / \mathrm{var} (E)$
[note that the fluctuation of the total energy due to MD integration error
should be ideally deducted from $\mathrm{var} (E)$].



\subsubsection{Pressure versus volume}

We can dynamically adjust the volume, $V$, of a system
in the canonical ensemble
so that the pressure can reach the target value, $p = p^*$.
%
%This permits simulation in the canonical ensemble
%as an alternative to the isothermal-isobaric ensemble approach.
%
Here volume, $V$, serves as a length scale parameter
of the simulation box,
and for a cubic box
the side length is given by $L = \sqrt[3]{V}$.
%
The coordinates are often scaled according to the scale, $L$,
such that reduced coordinates, defined as
$\mathbf R = \mathbf r / L$,
stays constant before and after a volume change.
%
To ensure the positivity of volume,
we change it by the logarithm, $X = \ln V$,
and Eq. \eqref{eq:invt_XY} specifies
the change in each step as
$$
\delta \ln V_t
=
\frac{1}{t} \cdot
\frac{ \beta \, p^* - \beta \, p_t }
     { \partial \, ( \beta \, p) / \partial \ln V }
,
$$
where we have included a factor of $\beta = 1/(k_B \, T)$
for convenience,
and the new volume is given by $V_{t+1} = V_t \, \exp(\delta \ln V_t)$.

For the canonical ensemble,
we have
%
\begin{align*}
  \beta \, p
  &=
  \frac{
    N_f + \beta \left\langle \mathrm{vir} \right\rangle
  } { 3 \, V }
  ,
  \\
  \frac{ \partial \, (\beta \, p) }
       { \partial \, (1/V) }
  &=
  \beta \, p \, V
  -
  \frac{\beta}{3}
  \left\langle
    \frac{ \partial \, \mathrm{vir} } { \partial \ln V }
  \right\rangle
  -
  \frac{\beta^2}{9}
  \left\langle
  \Delta( \mathrm{vir} )^2
  \right\rangle
  ,
\end{align*}
%
where
%$\beta = 1/(k_B \, T)$, and
the virial is defined as
%
\begin{equation*}
  \mathrm{vir}
  =
  \mathbf F \cdot \mathbf r
  =
  -3 \,
  \frac{ \partial \, U\left( \sqrt[3]{V} \, \mathbf R \right) }
       { \partial \, (\ln V) }
  .
\end{equation*}
%
Thus, we may choose the instantaneous pressure, $p_t$,
to be
%
\begin{equation*}
  \beta \, p_t
  =
  \frac{ N_f + \beta \, \mathrm{vir} }
       { 3 \, V }
  ,
\end{equation*}
%
and compute the derivative as
%
\begin{equation}
  \frac{ \partial \, ( \beta \, p ) }
       { \partial \, ( \ln V ) }
  =
  -\frac{
    \overline{ \beta \, p \, V }
  }{V}
  + \frac{\beta}{3 \, V} \,
      \overline{ \frac{ \partial \, \mathrm{vir} } { \partial \ln V } }
  + \frac{\beta^2}{9 \, V} \,
      \mathrm{var} ( \mathrm{vir} )
  .
  \label{eq:dbpdlnV}
\end{equation}
%
Note that we have included a factor of $V$
in the trajectory averages,
such as $\overline{\beta \, p \, V}$,
such that they
may tend to constants in the ideal-gas limit
even with a fluctuating volume.





\subsubsection{Chemical potential versus volume}

We can also adjust the volume to calibrate the chemical potential.
%
By Widom's insertion method,
the chemical potential can be computed from
trials of inserting a particle at random positions,
%
\begin{equation*}
  -\beta \, \mu
  =
  \ln \left[
  \frac{V}{N+1}
  \left\langle
    \exp(-\beta \, \Delta U)
  \right\rangle
  \right]
  ,
\end{equation*}
%
where $\Delta U$
is the increase of the potential energy
caused by adding a particle,
and we have ignored a constant shift due to the
momenta integral.
%
Thus, assuming small derivation from the target value, $\mu^*$,
the volume change in each step is given by
%
\begin{equation*}
  \delta \ln V_t
  =
  \frac{1}{t} \cdot
  \frac{
    %-\beta \, \mu^*  + \beta \, \mu_t
    1 - \frac{V}{N+1} \, \exp(-\beta \, \Delta U_t + \beta \, \mu^*)
  }
  {
    \partial \, (-\beta \, \mu) / \partial \, (\ln V)
  }
  .
\end{equation*}
%


We can obtain the derivative,
$\partial \, (-\beta \, \mu) / \partial \, (\ln V)$,
%with respect to volume change
by direct differentiation,
%
\begin{equation*}
  \frac{ \partial \, (-\beta \, \mu) }
       { \partial \, (\ln V) }
  =
  1 +
  \frac{\beta } { 3 } \,
  \left[
    \frac{ \langle \mathrm{vir}_+ \exp(-\beta \, \Delta U) \rangle }
         { \langle \exp(-\beta \, \Delta U) \rangle }
    -
    \langle \mathrm{vir} \rangle
  \right]
  ,
\end{equation*}
%
where $\mathrm{vir}_+ \equiv -3 \, \partial \, (U + \Delta U) / \partial \, (\ln V)$
stands for the virial
computed from the system with the additional particle.
%
However, for a single-component system,
it is more convenient and computationally more stable
to take advantage of the thermodynamic relation,
%$$
%\frac{ \partial \, P }
%     { \partial \, \rho }
%=
%\rho \,
%\frac{ \partial \, \mu }
%     { \partial \, \rho }
%,
%$$
%%
%and compute
\begin{equation}
  \frac{ \partial \, (-\beta \, \mu) }
       { \partial \, (\ln V) }
  =
  -\frac{V}{N}
  \frac{ \partial \, (\beta \, p) }
       { \partial \, (\ln V) }
  =
  \frac{ \partial \, (\beta \, p) }
       { \partial \, \rho }
  ,
  \label{eq:dmu_dp}
\end{equation}
%
and use Eq. \eqref{eq:dbpdlnV}
for $\partial \, (\beta\, p)/ \partial \, (\ln V)$.
%


\subsubsection{Variant of the Gibbs ensemble method}


We can combine the above two examples
to construct a variant of the Gibbs ensemble method\cite{
  panagiotopoulos1987, panagiotopoulos1988, frenkel}
of studying the liquid-gas phase coexistence.
%
We will simultaneously simulate a simple fluid
in two boxes, one in the gas and the other in the liquid phase.
%
We will adaptively adjust the volumes of the two boxes
to reach a point at which the two boxes share
the same pressure and chemical potential.
%
The updating formulas are
\begin{align}
  \delta \, \ln V_{1,t}
  &=
  \frac{1}{t} \cdot
  \frac{ \rho_2 \, \Delta( \beta \, \mu_t ) - \Delta( \beta \, p_t ) }
  { (\rho_2 - \rho_1) \, \partial (\beta \, p_1) / \partial \rho_1 }
  ,
  \label{eq:gibbs_p}
  \\
  \delta \, \ln V_{2,t}
  &=
  \frac{1}{t} \cdot
  \frac{ \rho_1 \, \Delta( \beta \, \mu_t ) - \Delta( \beta \, p_t ) }
  { (\rho_2 - \rho_1) \, \partial (\beta \, p_2) / \partial \rho_2 }
  ,
  \label{eq:gibbs_mu}
\end{align}
%
where
$\Delta( \beta \, p_t) \equiv \beta \, p_{t,1} - \beta \, p_{t,2})$,
$\Delta( \beta \, \mu_t) \equiv \beta \, \mu_{t,1} - \beta \, \mu_{t,2}$,
and
$\partial \, (\beta \, p) / \partial \rho$
can be found from Eq. \eqref{eq:dmu_dp}.
%
The details can be found in Appendix \ref{sec:gibbs}.

%
%Another example is the determination of the
%move size ($x$)
%in a Monte Carlo simulation
%in order to reaching a certain acceptance ratio ($y$).




\section{Results}



We coded the adaptive velocity scaling scheme to
the MD program NAMD\cite{NAMD},
and tested it on a box of water.
%
The system contains
$798$ TIP3P water molecules\cite{jorgensen1983}
in a cubic box of $30 \, \mathrm{\AA}$ side length.
%
Velocity scaling was conducted every $10$ steps
unless specified otherwise.
%
The MD time step was $2$ femtoseconds;
the temperature was $300$ K.
%
We used the particle-meshed Ewald method\cite{essmann1995}
with a spacing of $1 \, \mathrm{\AA}$
to compute the electrostatic interaction,
and the SETTLE method\cite{miyamoto1992}
to maintain the constraints.


We first compared the fluctuation of the total energy
from three simulations:
%
a regular MD without velocity scaling,
one under the regular velocity scaling scheme as described in the Introduction,
and one under adaptive velocity scaling.
%
Note that although Newton's equation conserves the total energy,
MD simulations usually employ approximate integration schemes
(such as the velocity Verlet),
with
small fluctuation of the total energy.
%
In all three cases,
we set the initial total energy to the correct value, $-6141.2$ kcal/mol.
%
In the second case,
velocity was scaled every $100$ or $1000$ MD steps.
%
In the last case,
velocity was scaled every $10$ MD steps with
$T'(E^*)$ computed from Eq. \eqref{eq:dT}.
%
As shown in Table \ref{tab:etraj},
the energy fluctuation from the simulation under adaptive velocity scaling
was similar to that from a regular MD,
which suggests that the influence of adaptive velocity scaling
becomes negligible for long simulations.
%
However, regular velocity scaling
produced much larger energy fluctuations
that increases with the scaling frequency.

\begin{table}[h]
  \setlength{\tabcolsep}{5pt}
  \renewcommand{\arraystretch}{1.5}
  \begin{center}
    \begin{tabular}{ p{2.5cm} | c p{1.3cm} p{1.3cm} c }
      \hline
      Velocity scaling
      &   None    &   Regular \newline $\nu =100 $   & Regular \newline $\nu = 1000$  &   Adaptive \\
      \hline
      $\sigma_E$ (kcal/mol)
      &   $0.32$  &   $20$    &  $7.8$     &   $0.23$ \\
      \hline
    \end{tabular}
  \end{center}
  \caption{
    \label{tab:etraj}
    Standard deviations of the total energy, $\sigma_E$,
    from simulations under different types of velocity scaling.
    %
    Each simulation lasted $10^7$ steps,
    and the standard deviation of the total energy,
    excluding the first $10^6$ steps, was reported.
    %
    The frequency of regular velocity scaling
    is denoted by $\nu$.
    %
    \note{
      The raw data were saved in
      \texttt{data/wb/fix/ene0fix.log},
      \texttt{data/wb/reg100/ene0reg.log},
      \texttt{data/wb/reg/ene0reg.log},
      and
      \texttt{data/wb/adp/ene0adp.log}.
      The standard deviations can be found with
      \texttt{make etraj -C data/wb}.
    }%
  }
\end{table}

%\begin{figure}[h]
%\begin{center}
%  \makebox[\linewidth][c]{
%    \includegraphics[angle=0, width=1.0\linewidth]{fig/etraj.pdf}
%  }
%  \caption{
%    \label{fig:etraj}
%    The time series of the total energy
%    from an MD trajectory
%    without velocity scaling (with the total energy $E \approx -6141.5$),
%    one under regular velocity scaling
%    (conducted every $10^3$ steps),
%    %(in which the velocity is scaled
%    %every $m = 10^4$ steps by a factor of
%    %$\sqrt{\frac12 N_f k_B T/\bar K}$,
%    %with $\bar K$ being the average kinetic energy
%    %of the past $m$ steps),
%    and
%    one under adaptive velocity scaling (conducted every $10$ steps).
%    %
%    Each simulation lasted $10^7$ steps,
%    but only the data from the first $3\times 10^6$ steps were shown.
%    %
%    %The points were plotted every $5 \times 10^4$ steps.
%    %
%    The standard deviations of the total energy,
%    excluding the first $10^6$ steps,
%    were $0.26$, $7.7$, and $0.25$ $\mathrm{kcal/mol}$,
%    respectively.
%    %
%    \note{The figure was produced by \texttt{doc/fig/etraj.gp}.
%      The raw data were saved in
%      \texttt{data/wb/fix/ene0fix.log},
%      \texttt{data/wb/reg/ene0reg.log},
%      and
%      \texttt{data/wb/adp/ene0adp.log}.
%      The standard deviations can be found with
%      \texttt{make etraj -C data/wb}.
%    }%
%  }
%\end{center}
%\end{figure}



We then compared the kinetic and potential energy
distributions from a regular MD simulation,
a simulation under adaptive velocity scaling,
a simulation under regular velocity scaling (performed every $100$ steps),
and
a simulation under the velocity rescaling
thermostat\cite{bussi2007}
(with $\tau = 0.03 \, \mathrm{ps}$).
%
The former two targets an (asymptotic) microcanonical ensemble,
whereas the last targets a canonical ensemble.
%
As shown in Fig. \ref{fig:kuhist},
adaptive velocity scaling and regular MD
produced almost identical distributions for the microcanonical ensemble,
while the simulation under the velocity rescaling thermostat
produced wider distributions for the canonical ensemble.
%
We may understand the widening % of the distributions
as a result of the construction of the canonical ensemble
as a superposition of microcanonical ensembles of different $E$'s.
%
Regular velocity scaling, however, produced
a potential energy distribution that
was visibly different from
those in the microcanonical and canonical ensembles.
%
%More interestingly,
%Fig. 2 shows that the adaptive velocity scaling
%yields shorter correlation for the kinetic energy.

\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/kuhist.pdf}
  }
  \caption{
    \label{fig:kuhist}
    Kinetic and potential energy distributions
    from a regular MD simulation in the microcanonical ensemble,
    a simulation under adaptive velocity scaling,
    one under regular velocity scaling,
    and one in the canonical ensemble.
    %
    %Each simulation had $10^7$ steps,
    %and the data from the first $10^6$ steps
    %were dropped.
    %
    \note{The figure was produced by \texttt{doc/fig/kuhist.gp}.
      For data preparation, \texttt{make uhist khist -C data/wb}.
    }%
  }
\end{center}
\end{figure}


Next, we wish to see if Eq. \eqref{eq:dE_final}
is indeed optimal in controlling the magnitude of velocity scaling.
%
To this end, we modified the energy change in
the adaptive velocity scaling scheme as
%such that the energy change in each step is given by
%
\begin{equation}
  \delta E_t
  =
  \frac{ \alpha(t) } { k_B \, T'(E^*) }
  \left(
   k_B \, T^* -
   \frac{ 2 \, K_t } { N_f }
  \right)
  ,
  \label{eq:dE_mod}
\end{equation}
%
where $\alpha(t) = z/t$ with $z$ being a free parameter;
and Eq. \eqref{eq:dE_final} is the $z = 1$ case.
%
For each value of $z$,
we performed multiple independent simulations of
$T = 10^5$ and $10^6$ steps,
and used the variance of the total energy at the simulation end
to represent the error.
%
For $T'(E^*)$,
Eq. \eqref{eq:dT_approx} was used with the heuristic value of
$\gamma = 0.33$,
which was obtained from Eq. \eqref{eq:gamma_def}
using the data of the first test.
%
As shown in Fig. \ref{fig:errz},
the value of $z = 1$
indeed gives the least error,
supporting the optimality of Eq. \eqref{eq:dE_final}.

\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/errz.pdf}
  }
  \caption{
    \label{fig:errz}
    Error of the total energy,
    measured as the fluctuation
    $\left\langle \Delta E^2(T) \right\rangle$,
    at the end of a simulation of $T$ steps,
    versus the variable $z$
    in the modified velocity scaling scheme,
    Eq. \eqref{eq:dE_mod}.
    %
    The points are results from averaging over
    multiple independent runs.
    %
    The curves are the analytical predictions from
    Eq. \eqref{eq:err_functional},
    for which we have computed the autocorrelation function
    of the fluctuation
    $\xi(t) = \left[T^* - \left( 2 K_t / N_f \right)\right]/\left[ k_B T'(E^*) \right]$
    from a regular MD simulation.
    %
    %The thinner curves were from a more approximate formula,
    %Eq. \eqref{eq:err_zovert},
    %which assumes the fluctuation is a white noise
    %%and this assumption is less reliable for shorter simulations
    %(cf. Appendix \ref{sec:error}).
    %
    \note{The figure was produced by \texttt{doc/fig/errz.gp}.
      The raw data were saved in \texttt{data/wb\_t100000.dat}.
      For the analytical prediction \texttt{make -C data/wb wberrz}.
    }%
  }
\end{center}
\end{figure}



Finally, we wish to test
the efficiency of the adaptive velocity scaling scheme
in recovering from a bad initial condition.
%
To this end,
we compared this scheme,
which targets the microcanonical ensemble,
with several temperature control schemes
targeting the canonical ensemble.
%
In this test, we initially lifted the total energy about
$100\,\mathrm{kcal/mol}$ above the average,
and monitored the evolution of the total energy.
%
As shown in Fig. \ref{fig:equil},
adaptive velocity scaling
behaved similarly to
the Nos\'e-Hoover chain\cite{nose1984, nose1984mp, hoover1985, martyna1992},
and velocity rescaling\cite{bussi2007}
thermostats
in the relaxation process;
and the velocity-scaling-based schemes
appeared to be more efficient than Langevin dynamics.
%
Note that the relaxation
of the thermostat algorithms
depends on some input parameters
(e.g., the inverse viscosity $\tau$ of the velocity rescaling thermostat),
whereas adaptive velocity scaling does not.

\note{
We also note that relaxation curve of the thermostat algorithms
take an exponential form ($\propto e^{-k\,t}$),
while that of adaptive velocity scaling takes
an algebraic form [$\propto 1/t$,
cf. \eqref{eq:abserr}].
%
One may wonder if it is more effective to use an exponential form
for the magnitude of velocity scaling,
$\alpha(t) \propto e^{-kt}$.
%
This is no so, as the alternative functional forms
would be less effective in the long run
in eliminating the cumulative random error
caused by velocity scaling itself,
i.e. the error bar of Fig. \ref{fig:equil} (not shown).
%
For the canonical ensemble, the fluctuation is unimportant
as the total energy is intrinsically fluctuating.
%
However, for the microcanonical ensemble, the fluctuation
is the key measure of convergence.
%
This is why adaptive velocity scaling chooses the $1/t$ form
instead of the exponential form for the scaling magnitude.
}

\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/equil.pdf}
  }
  \caption{
    \label{fig:equil}
    Time series of the total energy
    of simulations under
    adaptive velocity scaling,
    Langevin dynamics,
    Nos\'e-Hoover chain thermostat
    and
    velocity rescaling thermostat.
    %
    For Langevin dynamics,
    the damping parameter was $1.0 \, \mathrm{ps}^{-1}$.
    %
    For the Nos\'e-Hoover chain thermostat,
    we used $5$ chain variables. The masses
    of the first and the rest variables
    were $N_f \, \omega^2 \, k_B T$ and
    $\omega^2 \, k_B T$, respectively\cite{martyna1992},
    and
    $\omega = 2 \,\pi/(0.1 \, \mathrm{ps})$.
    %
    For the velocity rescaling thermostat,
    we tried two values of the inverse viscosity $\tau$,
    $0.1 \, \mathrm{ps}$ and $0.03 \, \mathrm{ps}$.
    %
    The average kinetic energy of the system
    was roughly $1426 \, \mathrm{kcal/mol}$.
    %
    The results were averaged over $10^4$ independent trials.
    %
    %\red{The lines are a guide to the eyes.}
    %
    \note{The figure was produced by \texttt{doc/fig/equil.gp}.
    }%
  }
\end{center}
\end{figure}






\section{\label{sec:conclusion}
Conclusions and Discussions}



In summary, we have presented an adaptive velocity scaling scheme
as a more precise temperature control for the microcanonical ($NVE$) ensemble.
%
Unlike the canonical ($NVT$) ensemble,
the microcanonical ensemble requires a constant total energy, $E$,
which is incompatible with the usual periodic velocity scaling scheme
with a fixed magnitude.
%
Our modified scheme gradually decreases the magnitude of the velocity scaling
by a factor of the number of steps.
%
Like the $1/t$ prescription\cite{
  belardinelli2007, *belardinelli2007jcp, *belardinelli2008,
  zhou2005, *zhou2008, *morozov2007}
for the WL algorithm\cite{wang2001, *wang2001pre},
the adaptive velocity scaling is able to reach
the desired temperature at an optimal rate.
%
Because of the magnitude control,
the effect of velocity scaling often becomes negligible
at long times so that
a simulation under adaptive velocity scaling
behaves similarly to a regular MD simulation.

We may view the adaptive velocity scaling scheme
as an application of the $1/t$ prescription
for adjusting the total energy
to achieve the target temperature.
%
More generally, we may apply this technique
to a pair of strongly correlated variables $x$ and $y$,
to find the optimal value of a parameter, $x$,
that corresponds to a target value of $y^*$.
%on the fly.
%
We expect this and similar parameter control schemes
to be useful in molecular simulations.




\section{Acknowledgment}

We thank Dr. J. Ma, Dr. M. W. Deem, Dr. Y. Mei,
O. Nassar, Dr. C. Lai, Dr. S. Ou and D. Stuhlsatz
for helpful discussions.
We gratefully acknowledge the Robert A. Welch Foundation (H-0037),
the National Science Foundation (CHE-1152876)
and
the National Institutes of Health (GM-037657) for partial support of this work.
%
%This research used computing resources of the National Science Foundation XSEDE grid.
We also acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas
at Austin for providing HPC and storage resources that have contributed to
the research results reported within this paper.

\appendix






\section{\label{sec:Tsurf}
Modification for the surface temperature}



The adaptive velocity rescaling in the main text
targets the bulk temperature.
%
Below we list a few changes that need to be made
for targeting the alternative surface definition.

According to the definition from
Eqs. \eqref{eq:DOSE} and \eqref{eq:betaE_surf_def},
we have
\begin{align}
  \beta_\Omega(E)
  &=
  \left\langle
    \frac{ \frac{1}{2}N_f - 1 } { K }
  \right\rangle
  ,
  \notag
  %\label{eq:betaE_invK}
  \\
  \beta'_\Omega(E)
  &=
  -
  \left\langle
    \frac{ \frac{1}{2} N_f - 1 } { K^2 }
  \right\rangle
  +
  \left\langle
    \Delta
    \left(
      \frac{ \frac{1}{2} N_f - 1 } { K }
    \right)^2
  \right\rangle
  ,
  \notag
\end{align}

The energy change in each velocity scaling step is given by
\begin{equation}
  \delta E_t
  =
  \frac{1}{t} \cdot
  \frac{ \beta^* - (\frac{1}{2} N_f - 1)/K_t }
  { \beta'_\Omega(E) }
  .
\end{equation}




\section{\label{sec:block}
Block average}

\newcommand{\tmax}{\mathcal T}

If block average is used, we have
%
\begin{align*}
  \sum_{t = 1}^{\tmax} \frac{ A_t } { t }
  &=
  \sum_{\tau = 1}^{\tmax/m}
  \sum_{\tau' = 1}^{m}
  \frac{ A_t } { (\tau - 1) \, m + \tau' }
  \\
  &\approx
  \sum_{\tau = 1}^{\tmax/m}
  \frac{ 1 } { \tau }
  \left(
    \frac 1 m
    \sum_{\tau' = 1}^m
    A_t
  \right)
  =
  \sum_{\tau = 1}^{\tmax/m}
  \frac{ A^*_\tau } { \tau }
  .
\end{align*}
%
where
$t = (\tau - 1) \, m + \tau'$
and
$
A_{\tau*} = \frac 1 m
\sum_{\tau' = 1}^m A_t.
$

In our case,
$
A_t = k_B T^* - \left( 2 K_t / N_f\right),
$
and the block average is
$
A_{\tau*}
= k_B T^* - \left( 2 K_{t*} / N_f\right),
$
where $K_{\tau*}$
is the block average of $K_t$.



\section{\label{sec:error}
  Optimal scaling magnitude
  from stochastic differential equation
  %Eq. \eqref{eq:dE_opt}
}

Here we present an alternative derivation of Eq. \eqref{eq:dE_opt}
using the method of stochastic differential equation.

Let $X_t$ be the value of the adjustable $X$ at step $t$,
then the aim of velocity scaling is to converge $X_t$
to the desired value of $X^*$.
%
We assume that we can form an independent estimate of $X^*$
in step $t$, $\mathcal X_t$,
which means, equivalently, an independent correction
to the current value of $X$ is given by
$\Delta \mathcal X_t = \mathcal X_t - X_{t-1}$.
%
%The correction can be used in velocity scaling
%to guide the amount of energy change.
%
If the independent correction were exact,
then it would suffice to perform
only one adjustment of $X$
with the change, $\delta X_t = X_t - X_{t-1}$,
being identical to the correction.
%
But since
the independent correction from an MD simulation
always contains random error,
the adjustment must be done continually.
%
We can, however, minimize the error, $X_\tmax$,
at the simulation end, $t = \tmax$,
by accepting the correction with some
optimal time-dependent magnitude, $\alpha(t)$,
as
%
\begin{equation}
  \delta X_t
  =
  \alpha(t) \, \Delta \mathcal X_t
  =
  \alpha(t) \, \left( \mathcal X_t - X_{t - 1} \right)
  .
  \label{eq:Xupdate}
\end{equation}
%
%Our aim is to minimize
%the expected error of the total energy, $X_\tmax$,
%at the end of the simulation, $t = \tmax$.

To find the optimal $\alpha(t)$,
we will switch to the continuous-time framework.
%
We define the error of the total energy at step $t$ as
$x(t) = X_{t-1} - X^*$,
then $\dot x(t) \approx \delta X_t$,
and Eq. \eqref{eq:Xupdate} can be approximated as
%
\begin{equation}
  \dot x(t)
  =
  -\alpha(t) \, x(t) + \alpha(t) \, \xi(t)
  ,
  \label{eq:x_diffeq}
\end{equation}
%
where $\xi(t) \equiv \mathcal X_t - X^*$
denotes the random fluctuation of the independent estimate at step $t$.
%
The solution of Eq. \eqref{eq:x_diffeq} is
%
\begin{equation}
  x(t)
  =
  x(1) \, e^{ -A(t) }
  +
  \int_1^t \dot u_t(\tau) \, \xi(\tau) \, d\tau
  ,
  %\label{eq:x_sol}
  \notag
\end{equation}
%
where
\begin{align*}
  A(t)
  &=
  \int_1^t \alpha(s) \, ds
  ,
\\
  u_t(\tau)
  &= \exp\left(
    -\int_\tau^t \alpha(s) \, ds
  \right)
  .
  \notag
\end{align*}
%
Thus, the average error at the end of simulation $t = \tmax$
is given by
%
\begin{align}
  \left\langle
    x(\tmax)
  \right\rangle
  &=
  \left\langle
    x(1)
  \right\rangle
  \, e^{ -A(t) }
  ,
  \label{eq:abserr}
\end{align}
%
and the square error
can be measured as
%
\begin{align}
  \left\langle
    x^2(\tmax)
  \right\rangle
  &=
  \left\langle
    x^2(1)
  \right\rangle
  \, e^{ -2 \, A(t) }
  \notag \\
  &+
  \int_1^{\tmax}
  \int_1^{\tmax}
  \dot u_{\tmax}(t) \, \dot u_{\tmax}(t') \,
  \left\langle
    \xi(t) \, \xi(t')
  \right\rangle
  \, dt \, dt'
  .
  \label{eq:err_functional}
\end{align}
%
Below we will find the optimal functional form of $\alpha(t)$
that minimizes the square error.



%\subsection{\label{sec:whitenoise}
%White-noise approximation
%}

For a very long simulation,
we may approximate $\xi(t)$ as an equivalent white noise,
such that
\begin{equation}
  \left\langle \xi(t) \, \xi(t') \right\rangle
  = \Gamma \, \delta(t - t')
  ,
  \notag
  %\label{eq:noise_corr}
\end{equation}
where
\begin{equation}
  \Gamma
  =
  \int_{-\infty}^\infty
  \left\langle
    \xi(t) \, \xi(0)
  \right\rangle
  \, dt
  =
  2 \, \tau \,
  \left\langle
    \xi^2(0)
  \right\rangle
  ,
  \label{eq:Gamma_def}
\end{equation}
with
$\tau_\xi$ being the integrated autocorrelation time
of $\left\langle \xi(t) \, \xi(0) \right\rangle$
%
Note that this is justifiable only
if the simulation length, $\tmax$, is much longer than
the autocorrelation time of $\xi(t)$,
as shown in Fig. \ref{fig:errz}.
%
Under this assumption,
Eq. \eqref{eq:err_functional} is simplified as
%
\begin{equation}
  \left\langle
    x^2(\tmax)
  \right\rangle
  =
  \left\langle
    x^2(1)
  \right\rangle
  \, e^{ -2 \, A(t) }
  +
  \Gamma
  \int_1^{\tmax}
    \dot u_{\tmax}^2(t) \, dt
  .
  \label{eq:err_functional_wn}
\end{equation}
%
This expression is a functional of $\alpha(t)$,
or equivalently, a functional of $u_{\tmax}(t)$.
%
To minimize it under a fixed value of $A(\tmax)$,
which implies fixed values of $u_{\tmax}(1)$ and $u_{t_m}(t_m)$,
we get, from the Euler-Lagrange equation,
$$
\dot u_{\tmax}(t) = \mathrm{const},
$$
which leads to the solution,
$u_{\tmax}(t) = (t + t_0) / (t_m + t_0)$,
with $c$ and $t_0$ being two constants.
%
As a result, we get the optimal scaling magnitude
%
\begin{equation}
  \alpha(t) = \frac{ 1 } { t + t_0 }
  ,
  \notag
  %\label{eq:alpha_opt}
\end{equation}
%
and the error is
%
$$%\begin{equation}
  \left\langle
    x^2(\tmax)
  \right\rangle
  =
  \frac{
    \left\langle x^2(1) \right\rangle
    \, (1 + t_0)^2
    + \Gamma \, (\tmax - 1)
  }
  {
    (\tmax + t_0)^2
  }
  ,
$$%\end{equation}

We can further determine the optimal value of $t_0$ as
$$
t_0 = \frac{ \Gamma } { \left\langle x^2(1) \right\rangle } - 1,
$$
%
with the minimal error being $\Gamma / (\tmax + t_0)$.
%
However, for a long simulation, the influence of $t_0$
is small, and for convenience we may approximately use
$$
\alpha(t) \approx \frac 1 t,
$$
which recovers Eq. \eqref{eq:dE_opt}.

\subsection{Constant time schedule}

The above framework also yields the saturation error
for an adaptive simulation under a constant updating magnitude,
in which the adjustable, $X$,
follows the generalized Berendsen algorithm,
Eq. \eqref{eq:Berendsen_XY},
instead of Eq. \eqref{eq:invt_XY}.
%
Although such simulations are non-ideal for asymptotic convergence,
they can help determine the values of $\Gamma$
and $Y'(X)$ of an unknown system.
%
Then the effective updating magnitude
is given by $\alpha(t) = k \, Y'(X)$.
%
In the asymptotic limit, $\tmax \to \infty$
we then have
from Eq. \eqref{eq:err_functional_wn},
%
\begin{equation}
  \mathrm{var}(X)
  =
  \left\langle x^2(\tmax) \right\rangle
  =
  \frac{1}{2} \, \Gamma \, k \, Y'(X)
  %=
  %\tau \, \left\langle \xi^2 \right\rangle \, k \, Y'(X)
  .
  \label{eq:varX_sat}
\end{equation}
%
On the other hand,
from the differential relation
$\delta Y \approx Y'(X) \delta X$,
we have
%
\begin{equation}
  \mathrm{var} (Y)
  \approx
  [Y'(x)]^2
  \left\langle
  \xi^2(0)
  \right\rangle
  .
  \label{eq:varY_varX}
\end{equation}
%
From Eqs. \eqref{eq:varX_sat}, \eqref{eq:varY_varX}, and \eqref{eq:Gamma_def}
we get Eq. \eqref{eq:dYdX_estimate}.


\subsection{Modified inverse-time schedule}

We can further show that Eq. \eqref{eq:dE_mod}
%corresponds to a modified schedule
%$\alpha(t) = z/t$,
results in an expected error of
\begin{equation}
  \left\langle
  x^2(\tmax)
  \right\rangle
  =
  \frac{ \langle x^2(1) \rangle } { \tmax^{2z} }
  +
  \frac{ \Gamma \, z^2 } { 2 \, z - 1 }
  \frac{
    \tmax^{2 \, z - 1} - 1
  }
  {
    \tmax^{2 \, z}
  }
  ,
  \notag
  %\label{eq:err_zovert}
\end{equation}
which is minimal at $z = 1$ for a long simulation.



\section{\label{sec:gibbs}
Variant of the Gibbs ensemble method
}

Eqs. \eqref{eq:gibbs_p} and \eqref{eq:gibbs_mu}
come from the solution of
%
\begin{align*}
  \frac{ -\Delta( \beta \, p_t ) } { t }
  &=
  \frac{ \partial \, (\beta \, p_1 ) }
       { \partial \, \ln V_1 }
  \,
  \delta \, \ln V_{1,t}
  -
  \frac{ \partial \, (\beta \, p_2 ) }
       { \partial \, \ln V_2 }
  \,
  \delta \, \ln V_{2,t}
  ,
  \\
  \frac{ \Delta ( \beta \, \mu_t ) } { t }
  &=
  \frac{ \partial \, (-\beta \, \mu_1 ) }
       { \partial \, \ln V_1 }
  \,
  \delta \, \ln V_{1,t}
  -
  \frac{ \partial \, (-\beta \, \mu_2 ) }
       { \partial \, \ln V_2 }
  \,
  \delta \, \ln V_{2,t}
  .
\end{align*}
%

\subsection{Virial and its derivative under a pairwise potential}

If the potential energy is pairwise,
$U = \sum_{i < j} \phi(r_{ij})$,
we have
%
\begin{align}
  \mathrm{vir}
  &=
  \sum_{i < j} \phi_1(r_{ij})
  ,
  \label{eq:vir_pair}
  \\
  -\frac{ \partial \, \mathrm{vir} }
        { \partial \ln V }
  &=
  \frac{1}{3}
  \sum_{i < j} \phi_2(r_{ij})
  ,
  \label{eq:dvir_pair}
\end{align}
%
where $\phi_1(r) = - r \, \phi'(r)$
and $\phi_2(r) = -r \, \phi'_1(r)$.
%
For the Lennard-Jones potential, particularly, we have
%
\begin{align*}
  \phi(r)
  &=
  4 \, \epsilon
  \left[
    \left( \frac{\sigma}{r} \right)^{12}
    -
    \left( \frac{\sigma}{r} \right)^{6}
  \right]
  ,
  \\
  \phi_1(r)
  &=
  4 \, \epsilon
  \left[
    12
    \left( \frac{\sigma}{r} \right)^{12}
    -
    6
    \left( \frac{\sigma}{r} \right)^{6}
  \right]
  ,
  \\
  \phi_2(r)
  &=
  4 \, \epsilon
  \left[
    144
    \left( \frac{\sigma}{r} \right)^{12}
    -
    36
    \left( \frac{\sigma}{r} \right)^{6}
  \right]
  .
\end{align*}

In simulating a finite system,
we often have to truncate the potential at a cutoff distance,
$r_c$, and thus need to include long-range tail corrections
for Eqs. \eqref{eq:vir_pair} and \eqref{eq:dvir_pair},
which can be written as
%
$$
  \frac{ 2 \, \pi \, N^2 } { V }
  \left(
    \frac{ C_{12} } { 9 \, C_9 }
    -
    \frac{ C_6 } { 3 \, r_c^3 }
  \right)
  ,
$$
%
where $C_6/(\epsilon \, \sigma^6)$ is $4$, $24$ and $48$,
for $U$, $-\mathrm{vir}$ and $-\partial\,\mathrm{vir}/\partial \ln V$,
respectively,
and $C_{12}/(\epsilon \, \sigma^{12})$ is
$4$, $48$ and $192$, respectively.


%\bibliographystyle{abbrv}
\bibliography{simul}
\end{document}
