%\documentclass[12pt]{article}
\documentclass[reprint]{revtex4-1}
\usepackage{amsmath}
\usepackage{mathtools}
\usepackage{upgreek}
\usepackage[table,usenames,dvipsnames]{xcolor}
\usepackage{hyperref}

\hypersetup{
  colorlinks,
  linkcolor={red!30!black},
  citecolor={green!20!black},
  urlcolor={blue!80!black}
}


\definecolor{DarkBlue}{RGB}{0,0,64}
\definecolor{DarkBrown}{RGB}{64,20,10}
\definecolor{DarkGreen}{RGB}{0,64,0}
\definecolor{DarkPurple}{RGB}{64,0,42}
\definecolor{LightGray}{gray}{0.85}
% annotation macros
\newcommand{\red}[1]{{\color{red} #1}}
\newcommand{\repl}[2]{{\color{gray} [#1] }{\color{blue} #2}}
\newcommand{\add}[1]{{\color{blue} #1}}
\newcommand{\del}[1]{{\color{gray} [#1]}}
\newcommand{\note}[1]{{\color{DarkGreen}\footnotesize \textsc{Note.} #1}}
\newcommand{\answer}[1]{{\color{DarkBlue}\footnotesize \textsc{Answer.} #1}}
\newcommand{\summary}[1]{{\color{DarkPurple}\footnotesize \textsc{Summary.} #1}}



\begin{document}



\title{Adaptive velocity scaling for an asymptotic
microcanonical ensemble}
\author{}
%\date{\vspace{-7ex}}
\begin{abstract}
  We present a modification of the commonly-used
  temperature control scheme: velocity scaling.
  %
  The usual velocity scaling scheme produces a perturbation to
  the microcanonical ($NVE$) ensemble
  as the scaling operation causes the total energy to fluctuate continually.
  %
  Inspired by the $1/t$ prescription for improving the Wang-Landau algorithm,
  we modify the usual velocity scaling scheme
  by reducing the scaling magnitude by the number of steps.
  %
  In this way, the fluctuation of total energy quickly becomes negligible,
  and the simulation ensemble becomes effectively a microcanonical one.
  %
  Further, we shall show that the reduction factor
  allows the microcanonical temperature to tend to the target value
  at an optimal rate.
\end{abstract}

\maketitle



Velocity scaling is a temperature control scheme
for molecular dynamics (MD) simulations.
%
In a common version,
the velocity is regularly scaled such that the average
kinetic energy of the past few steps, $\bar K$,
can match the expected value\cite{frenkel},
%
\begin{equation}
  K^* = \frac{1}{2} N_f \, k_B \, T
  ,
  \label{eq:K_canon}
\end{equation}
%
where $N_f$, $k_B$, and $T$ are the number of degrees of freedom,
the Boltzmann constant, and temperature, respectively.
%
We refer to this version as the regular velocity scaling below.
%
A somewhat gentler modification, the Berendsen thermostat\cite{berendsen1984},
modifies the scaling scheme, such that
the kinetic energy approaches the target value at a rate of
$\left( K^* - \bar K \right)/\tau$,
with $\tau^{-1}$ being the rate of damping.

The above schemes, albeit effective,
do not result in exact sampling for the microcanonical ($NVE$) ensemble\cite{hermansson1988},
because they cause the total energy to fluctuate continually.
%
Thus, they are often regarded as expedient protocols
suitable only for parameter tuning in preliminary runs,
but not for rigorous MD simulations in the microcanonical ensemble.
%
On the other hand, in several ingenious modifications,
the energy fluctuation of velocity scaling
can be used to design thermostat algorithms
for the canonical ($NVT$) ensemble, in which the total energy
is intrinsically fluctuating.
%
These modifications,
exemplified by the Nos\'e-Hoover\cite{nose1984, nose1984mp, hoover1985, martyna1992}
and Langevin-style velocity rescaling\cite{bussi2007} thermostats,
are rigorous algorithms, as
the velocity scaling operation is carefully
controlled to make
the resulting fluctuation of the total energy
mimic the natural behavior demanded by the underlying canonical distribution.
%
Motivated by the thermostat algorithms,
one may wish to consider similar modifications
that result in precise temperature control
for the microcanonical ensemble.
%Albeit the success and popularity of the thermostat algorithms,
%one may argue that these algorithms often introduce
%some artificial time scales that are foreign to the natural time evolution.

Here we present a modified velocity scaling scheme
for an asymptotic microcanonical ensemble,
in which
the total energy is gradually stabilized to a constant
that corresponds to the target temperature.
%
Our modification is inspired by the $1/t$ prescription\cite{
  belardinelli2007, belardinelli2007jcp, belardinelli2008,
  zhou2005, zhou2008, morozov2007}
for improving the convergence of
the Wang-Landau (WL) algorithm\cite{
  wang2001, wang2001pre}.
%
The WL algorithm,
which falls in a larger class of free energy methods
of metadynamics\cite{
  laio2002, laio2008, marsili2006},
has parameters for the entropy that need to be
determined and updated adaptively,
and the $1/t$ prescription gives the optimal updating magnitude
for the parameters to reach their target values.
%
By analogy, we may compare the total energy here
to a parameter in the WL algorithm,
and set the optimal magnitude
of velocity scaling to be inversely proportional
to the number of steps.
%
Such a modification could hopefully gradually decrease
the fluctuation of the total energy, and
help the total energy tend to the target value in an optimal rate.
%
Besides the above change,
our modification will target a more accurate
microcanonical temperature\cite{rugh1997},
which differs from the usual definition, Eq. \eqref{eq:K_canon},
for the canonical ensemble.
%
As we shall see,
with a rapidly decreasing magnitude of velocity scaling,
a sufficiently long simulation under adaptive velocity scaling
is almost identical to a regular MD
in conserving the total energy and producing other quantities
of the microcanonical ensemble.



\section{Methods}



\subsection{Background in Statistical Mechanics}



Our adaptive velocity scaling scheme aims at
controlling the temperature for an
MD simulation in the microcanonical ensemble.
%
Since the microcanonical ensemble is parameterized by
the total energy, $E$,
our scheme must adjust the total energy
to influence the temperature of the ensemble.


An MD trajectory generated from
Newton's equation of motion
can be thought as a realization
of a microcanonical ensemble,
or a collection of configurations
with the same total energy.
%
Below we shall denote a configuration in the trajectory
by a vector $\mathbf x$,
specifying both the coordinates and momenta.
%
Under the ergodicity hypothesis,
Newton's equation will populate configurations
that spread evenly over
the isoenergetic hypersurface in the phase space,
defined by
$H(\mathbf x) = E$,
where $H(\mathbf x)$ is the Hamiltonian function.
%
The area of the hypersurface gives the density of states,
%
\begin{equation}
  \Omega(E)
  =
  \int
    \delta\bigl( H(\mathbf x) - E \bigr)
    \, d\mathbf x
  ,
  \notag
  %\label{eq:omegaE}
\end{equation}
%
and the microcanonical temperature,
defined via the inverse temperature,
$\beta = 1/(k_B T)$,
is given by
%
\begin{equation}
  \beta(E)
  =
  \frac{ d }{ dE }
  \ln \Omega(E)
  =
  \frac{ \Omega'(E) }
       { \Omega(E)  }
  ,
  \label{eq:betaE_def}
\end{equation}
%
and is, therefore, a function of $E$.
%

By velocity scaling, we want to
find a value of the total energy, $E^*$,
such that the microcanonical temperature
is equal to the target value, $\beta^*$,
i.e., the solution of
%
\begin{equation}
  \beta(E^*)
  =
  \beta^*
  .
  \label{eq:beta_star}
\end{equation}
%
Unfortunately, the temperature function, $\beta(E)$,
is usually unknown in advance
and must be learned gradually from simulation.
%
So is the solution of Eq. \eqref{eq:beta_star}.
%
Below, we review an adaptive averaging technique
which offers a way of gradually improving
our knowledge of the temperature function,
and hence the precision of the target energy, $E^*$.



\subsection{Adaptive averaging}



Suppose that in each MD step, $t$,
we may form an independent estimate of $E^*$,
$\mathcal E_t$,
then a more reliable estimate of $E^*$
would be the average of
all previous independent estimates,
%
\begin{equation}
  \bar{\mathcal E}_t
  =
  \frac 1 t
  \sum_{\tau = 1}^t
    \mathcal E_\tau
  .
  \label{eq:Epsave}
\end{equation}
%
This is a runtime average,
and its precision improves over the simulation course.



Our adaptive velocity scaling is based on
the above runtime averaging,
and it differs from the regular velocity scaling
in that we use $\bar{\mathcal E}_t$
instead of $\mathcal E_t$
to guide the amount of energy change in each step.
%
That is, we want
the total energy of the system
at the end of each step
to be given by
the runtime average, $\bar{\mathcal E}_t$.
%instead of $\mathcal E_t$.
%
This condition requires the amount of energy increment
by velocity scaling in step $t$ to be
%
\begin{equation}
  \delta E_t
  =
  \bar{\mathcal E}_t - \bar{\mathcal E}_{t - 1}
  .
  \label{eq:dE_adaptive}
\end{equation}

As the runtime average from Eq. \eqref{eq:Epsave}
stabilizes, the magnitude of velocity scaling
naturally decreases.
%
This can be seen by rewriting Eq. \eqref{eq:Epsave}
as a recurrence relation
%
\begin{align}
  \bar{\mathcal E}_t
  &=
  \frac{1}{t}
  \left[
    (t - 1) \, \bar{\mathcal E}_{t - 1}
    + \mathcal E_t
  \right]
  \notag \\
  &=
  \bar{\mathcal E}_{t - 1}
  +
  \frac{
    \mathcal E_t - \bar{\mathcal E}_{t - 1}
  }
  {
    t
  }
  .
\label{eq:Epsave_recur}
\end{align}
%
Here we may understand $\mathcal E_t - \bar{\mathcal E}_{t - 1}$
as the independent correction
to the current average $\bar{\mathcal E}_{t - 1}$
from the configuration at step $t$,
and Eq. \eqref{eq:Epsave_recur} shows that
when the correction is absorbed in the runtime average
it carries the weight of $1/t$.
%
From Eqs. \eqref{eq:dE_adaptive} and \eqref{eq:Epsave_recur},
we obtain the optimal amount of energy change
for each MD step
\begin{align}
  \delta E_t
  =
  \frac{ 1 } { t }
  \cdot
  \left( \mathcal E_t - \bar{\mathcal E}_{t - 1} \right)
  .
  \label{eq:dE_opt}
\end{align}
%
This amount of energy change is to be realized by velocity scaling.
%
Equation \eqref{eq:dE_opt} is analogous to the $1/t$ prescription\cite{
  belardinelli2007, belardinelli2007jcp, belardinelli2008,
  zhou2005, zhou2008, morozov2007}
for the optimal control of the updating magnitude
of the WL algorithm\cite{wang2001, wang2001pre},
and the above derivation results from its close connection\cite{
  marsili2006, barducci2008}
to the adaptive biasing force method\cite{darve2001, darve2008}.
%
In Appendix \ref{sec:error}, we give an alternative derivation of the result.



\subsection{Linearization}



Next, we need to form an independent estimate of
$\mathcal E_t$ of $E^*$.
%
For this, we shall form
an independent estimate of temperature, $\beta_t$
(detailed in the next subsection)
and solve a linearized version of Eq. \eqref{eq:beta_star}.

Linearizing Eq. \eqref{eq:beta_star} around the solution
yields
%
\begin{equation*}
\beta^*
\approx
\beta( E )
+
\beta'( E^* ) \, ( E^* - E )
,
%\label{eq:beta_linear}
\end{equation*}
%
or
%
\begin{equation}
E^*
\approx
E
+
\frac{ \beta^* - \beta(E) }
     { \beta'(E^*) }
.
\label{eq:E_inversion}
\end{equation}
%
In adaptive velocity scaling,
we require the total energy at the beginning of every step
(before scaling) to match the previous runtime average,
$\bar{\mathcal E}_{t - 1}$.
%
Thus, if we use $\bar{\mathcal E}_{t - 1}$ for $E$
and $\beta_t$ for $\beta(E)$
in Eq. \eqref{eq:E_inversion},
$E^*$ would be the independent estimate
$\mathcal E_{t}$ at step $t$:
%
%
\begin{equation}
\mathcal E_t
=
\bar{\mathcal E}_{t - 1}
+
\frac{ \beta^* - \beta_t }
     { \beta'(E^*) }
.
\label{eq:Eps}
\end{equation}
%
Thus, the optimal energy increment,
from Eqs. \eqref{eq:dE_opt} and \eqref{eq:Eps},
is
%
\begin{equation}
\delta E_t
=
\frac{ 1 } { t }
\cdot
\frac{ \beta^* - \beta_t }
     { \beta'(E^*) }
.
\label{eq:dE_beta}
\end{equation}
%



\subsection{Microcanonical temperature}


We now give formulas for $\beta_t$
and $\beta'(E)$ required in Eq. \eqref{eq:dE_beta}.
%
It can be shown that both the microcanonical temperature, $\beta(E)$,
and $\beta'(E)$ can be evaluated as
averages in the microcanonical ensemble\cite{rugh1997, frenkel}:
%
\begin{equation}
  \beta(E)
  =
  \left\langle
    \frac{ N_f - 2 }
         { 2 \, K }
  \right\rangle_E
  ,
  \label{eq:betaE_invK}
\end{equation}
%
and
%
\begin{equation}
  \beta'(E)
  =
  - \left\langle
      \frac{ N_f - 2 }
           { 2 \, K^2 }
    \right\rangle_E
  + \left\langle
      \Delta\left(
        \frac{ N_f - 2 }
             { 2 \, K }
      \right)^2
    \right\rangle_E
  ,
  \label{eq:dbetadE}
\end{equation}
%
where
$\langle \Delta A^2 \rangle = \langle A^2 \rangle - \langle A \rangle^2$
means the variance of quantity $A$.
%
%We will use Eq. \eqref{eq:dbetadE} to gauge
%the magnitude of adjusting the velocity-scaling factor.
Note that the usual definition of temperature, Eq. \eqref{eq:K_canon},
however, is derived from the canonical ensemble and
only approximate for the microcanonical ensemble.



If we use the quantity in the brackets of
Eq. \eqref{eq:betaE_invK} for the inverse temperature
$\beta_t$,
then the optimal energy increment is given by
Eq. \eqref{eq:dE_beta}:
%
\begin{equation}
\delta E_t
=
\frac{ 1 } { t } \cdot
\frac{ 1 } { \beta'(E^*) }
\left(
 \beta^* -
 \frac{ N_f - 2  }
      { 2 \, K_t }
\right)
,
\label{eq:dE_final}
\end{equation}
%
where
$K_t$ is the kinetic energy before velocity scaling at step $t$,
and
Eq. \eqref{eq:dbetadE} can be used to approximate
$\beta'(E^*)$ as
%
\begin{align}
  \beta'(E^*)
  \approx
  - \overline{
    \left(
      \frac{ N_f - 2 }
           { 2 \, K_t^2 }
    \right)
    }
    +
    \operatorname{Var}
    \left(
        \frac{ N_f - 2 }
             { 2 \, K_t }
    \right)
  ,
  \label{eq:dbeta}
\end{align}
%
where
$\overline A$ means the trajectory average of $A$,
and
$\operatorname{Var}(A) = \overline{ A^2 } - {\overline A}^2$
is the corresponding variance.

In practice, velocity scaling is often implemented
only every $m$ MD steps.
%
Then,
as shown in Appendix \ref{sec:block},
we can interpret the $t$ in Eq. \eqref{eq:dE_final} as
the number of velocity-scaling operations instead of
the number of MD steps (i.e., $t \to t/m$), and
the $1/K_t$ as the average inverse kinetic energy
of the $m$ steps right
before the scaling operation.




\subsection{Approximations on $\beta'(E^*)$}


Ideally, we should have $\beta'(E) \le 0$
as the temperature $T$ increases the total energy $E$.
%
This, however, may not be true in practice
because the second variance term on the right-hand side
of Eq. \eqref{eq:dbeta}
can be susceptible to numerical error.
%
%Further, since the variance is nonnegative, we have
%%
%\begin{equation*}
%  - \overline{
%    \left(
%      \frac{ N_f - 2 }
%           { 2 \, K_t^2 }
%    \right)
%    }
%  \le
%  \beta'(E^*)
%  \le
%  0,
%\end{equation*}
%
To avoid the variance term,
we may define a ratio
%
\begin{equation}
  \gamma
  \equiv
  \frac
  {
    -\beta'(E^*)
  }
  {
    \overline{
      \left( \frac 1 2 N_f - 1  \right) / K_t^2
    }
  }
  ,
  \notag
  %\label{eq:gamma_def}
\end{equation}
%
and rewrite Eq. \eqref{eq:dbeta} as
%
\begin{equation}
  \beta'(E^*)
  =
  -\gamma \, \overline{
    \left(
      \frac{ N_f - 2 }
           { 2 \, K_t^2 }
    \right)
    }
  .
  \label{eq:dbeta_approx}
\end{equation}
%
For a physical system, we expect $0 \le \gamma \le 1$.
%
%The value of $\gamma$ is computed for a model system
%in Appendix \ref{sec:model}.

We may use Eq. \eqref{eq:dbeta_approx} in two ways.
%
First, as a safeguard,
we may propose a minimal value of $\gamma$, say $0.1$,
to prevent an overly large value from Eq. \eqref{eq:dbeta}.

Second,
we can determine an approximate heuristic value of $\gamma$
that is applicable for a class of similar systems.
%
Then,
from Eqs. \eqref{eq:dbeta}-\eqref{eq:dbeta_approx},
we have
%
\begin{equation}
  \beta'(E^*)
  =
  -\frac{ \gamma \, {\beta^*}^2 }
  { \frac{1}{2} N_f + \gamma - 2 }
  ,
  \label{eq:dbeta_approx2}
\end{equation}
%
which requires no averaging.
%
For example,
for an explicit solvent simulation of TIP3P water\cite{jorgensen1983}
at 300K, we find that the optimal $\gamma$
to be around $0.33$.



\section{Results}



We coded the adaptive velocity scaling scheme to
the MD program NAMD\cite{NAMD},
and tested it on a box of water.
%
The system contains
$798$ TIP3P water molecules\cite{jorgensen1983}
in a cubic box of $30 \, \mathrm{\AA}$ side length.
%
Velocity scaling was conducted every $10$ steps
unless specified otherwise.
%
The MD time step was $2$ femtoseconds;
the temperature was $300$ K.
%
We used the particle-meshed Ewald method\cite{essmann1995}
with a spacing of $1 \, \mathrm{\AA}$
to compute the electrostatic interaction,
and the SETTLE method\cite{miyamoto1992}
to maintain the constraints.


We first compared the fluctuation of the total energy
from three simulations:
%
a regular MD without velocity scaling,
one under the regular velocity scaling scheme as described in the Introduction,
and one under adaptive velocity scaling.
%
Note that although Newton's equation conserves the total energy,
MD simulations usually employ approximate integration schemes
(such as the velocity Verlet),
which lead to small fluctuation of the total energy.
%
In all three cases,
we set the initial total energy to the correct value, $-6141.2$ kcal/mol.
%
In the second case,
velocity was scaled every $10^3$ MD steps.
%
In the last case,
velocity was scaled every $10$ MD steps with
$\beta'(E^*)$ computed from Eq. \eqref{eq:dbeta}.
%
As shown in Table \ref{tab:etraj},
the energy fluctuation from the simulation under adaptive velocity scaling
was similar to that from a regular MD,
which suggests that the influence of adaptive velocity scaling
becomes negligible for long simulations.
%
However, regular velocity scaling
produced a much larger energy fluctuation
despite a low scaling frequency.

\begin{table}[h]
  \setlength{\tabcolsep}{5pt}
  \renewcommand{\arraystretch}{1.5}
  \begin{center}
    \begin{tabular}{ p{2.5cm} | c c c }
      \hline
      Velocity scaling
      &   None    &   Regular   &   Adaptive \\
      \hline
      $\sigma_E$ (kcal/mol)
      &   $0.26$  &   $7.7$     &   $0.25$ \\
      \hline
    \end{tabular}
  \end{center}
  \caption{
    \label{tab:etraj}
    Standard deviations of the total energy, $\sigma_E$,
    from simulations under different types of velocity scaling.
    %
    Each simulation lasted $10^7$ steps,
    and the standard deviation of the total energy
    of the last $9 \times 10^6$ steps was reported.
    %
    \note{
      The raw data were saved in
      \texttt{data/wb/fix/ene0fix.log},
      \texttt{data/wb/reg/ene0reg.log},
      and
      \texttt{data/wb/adp/ene0adp.log}.
      The standard deviations can be found with
      \texttt{make etraj -C data/wb}.
    }%
  }
\end{table}

%\begin{figure}[h]
%\begin{center}
%  \makebox[\linewidth][c]{
%    \includegraphics[angle=0, width=1.0\linewidth]{fig/etraj.pdf}
%  }
%  \caption{
%    \label{fig:etraj}
%    The time series of the total energy
%    from an MD trajectory
%    without velocity scaling (with the total energy $E \approx -6141.5$),
%    one under regular velocity scaling
%    (conducted every $10^3$ steps),
%    %(in which the velocity is scaled
%    %every $m = 10^4$ steps by a factor of
%    %$\sqrt{\frac12 N_f k_B T/\bar K}$,
%    %with $\bar K$ being the average kinetic energy
%    %of the past $m$ steps),
%    and
%    one under adaptive velocity scaling (conducted every $10$ steps).
%    %
%    Each simulation lasted $10^7$ steps,
%    but only the data from the first $3\times 10^6$ steps were shown.
%    %
%    %The points were plotted every $5 \times 10^4$ steps.
%    %
%    The standard deviations of the total energy,
%    excluding the first $10^6$ steps,
%    were $0.26$, $7.7$, and $0.25$ $\mathrm{kcal/mol}$,
%    respectively.
%    %
%    \note{The figure was produced by \texttt{doc/fig/etraj.gp}.
%      The raw data were saved in
%      \texttt{data/wb/fix/ene0fix.log},
%      \texttt{data/wb/reg/ene0reg.log},
%      and
%      \texttt{data/wb/adp/ene0adp.log}.
%      The standard deviations can be found with
%      \texttt{make etraj -C data/wb}.
%    }%
%  }
%\end{center}
%\end{figure}



We then compared the kinetic and potential energy
distributions from a regular MD simulation,
a simulation under adaptive velocity scaling,
and
a simulation under the velocity rescaling thermostat\cite{bussi2007}
(with $\tau = 0.03 \, \mathrm{ps}$).
%
The former two targets an (asymptotic) microcanonical ensemble,
whereas the last targets a canonical ensemble.
%
As shown in Fig. \ref{fig:kuhist},
adaptive velocity scaling and regular MD
produced almost identical distributions,
while the simulation under the velocity rescaling thermostat
produced wider distributions.
%
We may understand the widening % of the distributions
as a result of the construction of the canonical ensemble
as a superposition of microcanonical ensembles of different $E$'s.
%
%The distribution from regular velocity scaling,
%however, was visibly different from
%that from the microcanonical or canonical ensemble.
%
%More interestingly,
%Fig. 2 shows that the adaptive velocity scaling
%yields shorter correlation for the kinetic energy.

\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/kuhist.pdf}
  }
  \caption{
    \label{fig:kuhist}
    Kinetic and potential energy distributions
    from a regular MD simulation in the microcanonical ensemble,
    a simulation under adaptive velocity scaling,
    and one in the canonical ensemble.
    %
    %Each simulation had $10^7$ steps,
    %and the data from the first $10^6$ steps
    %were dropped.
    %
    \note{The figure was produced by \texttt{doc/fig/kuhist.gp}.
      For data preparation, \texttt{make uhist khist -C data/wb}.
    }%
  }
\end{center}
\end{figure}


Next, we wish to see if Eq. \eqref{eq:dE_final}
is indeed the optimal in controlling the magnitude of velocity scaling.
%
To this end, we modified the adaptive velocity scaling scheme
such that the energy change in each step is given by
%
\begin{equation}
  \delta E_t
  =
  \frac{ \alpha(t) } { \beta'(E^*) }
  \left(
   \beta^* -
   \frac{ N_f - 2 }
   { 2 \, K_t }
  \right)
  ,
  \label{eq:dE_mod}
\end{equation}
%
where $\alpha(t) = z/t$ with $z$ being a free parameter;
and Eq. \eqref{eq:dE_final} is the $z = 1$ case.
%
For each value of $z$,
we performed multiple independent simulations of
$T = 10^5$ and $10^6$ steps,
and used the variance of the total energy at the simulation end
to represent the error.
%
For $\beta'(E^*)$,
Eq. \eqref{eq:dbeta_approx2} was used with the heuristic value of $0.33$.
%
As shown in Fig. \ref{fig:errz},
the value of $z = 1$
indeed gives the least error,
supporting the optimality of Eq. \eqref{eq:dE_final}.

\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/errz.pdf}
  }
  \caption{
    \label{fig:errz}
    Error of the total energy,
    measured as the fluctuation
    $\left\langle \Delta E^2(T) \right\rangle$,
    at the end of a simulation of $T$ steps,
    versus the variable $z$
    in the modified velocity scaling scheme,
    Eq. \eqref{eq:dE_mod}.
    %
    The points are results from averaging over
    multiple independent runs.
    %
    The curves are the analytical predictions from
    Eq. \eqref{eq:err_functional},
    for which we have computed the autocorrelation function
    of the fluctuation
    $\xi(t) = \left[\beta^* - \left(\frac 1 2 N_f-1\right)/K_t\right]/\beta'(E^*)$
    from a regular MD simulation.
    %
    %The thinner curves were from a more approximate formula,
    %Eq. \eqref{eq:err_zovert},
    %which assumes the fluctuation is a white noise
    %%and this assumption is less reliable for shorter simulations
    %(cf. Appendix \ref{sec:error}).
    %
    \note{The figure was produced by \texttt{doc/fig/errz.gp}.
      The raw data were saved in \texttt{data/wb\_t100000.dat}.
      For the analytical prediction \texttt{make -C data/wb wberrz}.
    }%
  }
\end{center}
\end{figure}



Finally, we wish to test
the efficiency of the adaptive velocity scaling scheme
in recovering from a bad initial condition.
%
To this end,
we compared this scheme,
which targets the microcanonical ensemble,
with several temperature control schemes
targeting the canonical ensemble.
%
In this test, we lifted the total energy about
$100\,\mathrm{kcal/mol}$ above the average,
and monitored the evolution of the total energy.
%
As shown in Fig. \ref{fig:equil},
adaptive velocity scaling
behaved similarly to
the Nos\'e-Hoover chain\cite{nose1984, nose1984mp, hoover1985, martyna1992},
and velocity rescaling\cite{bussi2007}
thermostats
in the relaxation;
and these velocity-scaling-based schemes
appeared to be more efficient than Langevin dynamics.
%
Note that the relaxation time
of the thermostat algorithms
depends on some input parameters
(e.g., the inverse viscosity $\tau$ of the velocity rescaling thermostat),
whereas adaptive velocity scaling does not.

\note{
We also note that relaxation curve of the thermostat algorithms
take an exponential form ($\propto e^{-k\,t}$),
while that of adaptive velocity scaling takes
an algebraic form [$\propto 1/t$,
cf. \eqref{eq:abserr}].
%
One may wonder if it is more effective to use an exponential form
for the magnitude of velocity scaling,
$\alpha(t) \propto e^{-kt}$.
%
This is no so, as the alternative functional forms
would be less effective in the long run
in eliminating the random error caused by velocity scaling,
i.e., the error bar of Fig. \ref{fig:equil},
which is not shown.
%
For the canonical ensemble, the fluctuation is unimportant
as the total energy is intrinsically fluctuating.
%
However, for the microcanonical ensemble, the fluctuation
is the key measure of convergence.
%
This is why adaptive velocity scaling chooses the $1/t$ form
instead of the exponential form for the scaling magnitude.
}

\begin{figure}[h]
\begin{center}
  \makebox[\linewidth][c]{
    \includegraphics[angle=0, width=1.0\linewidth]{fig/equil.pdf}
  }
  \caption{
    \label{fig:equil}
    Time series of the total energy
    of simulations under
    adaptive velocity scaling,
    Langevin dynamics,
    Nos\'e-Hoover chain thermostat
    and
    velocity rescaling thermostat.
    %
    For Langevin dynamics,
    the damping parameter was $1.0 \, \mathrm{ps}^{-1}$.
    %
    For the Nos\'e-Hoover chain thermostat,
    we used $5$ chain variables. The masses
    of the first and the rest variables
    were $N_f \, \omega^2 \, k_B T$ and
    $\omega^2 \, k_B T$, respectively\cite{martyna1992},
    and
    $\omega = 2 \,\pi/(0.1 \, \mathrm{ps})$.
    %
    For the velocity rescaling thermostat,
    we tried two values of the inverse viscosity $\tau$,
    $0.1 \, \mathrm{ps}$ and $0.03 \, \mathrm{ps}$.
    %
    The average kinetic energy of the system
    was roughly $1426 \, \mathrm{kcal/mol}$.
    %
    The results were averaged over $10^4$ independent trials.
    %
    %\red{The lines are a guide to the eyes.}
    %
    \note{The figure was produced by \texttt{doc/fig/equil.gp}.
    }%
  }
\end{center}
\end{figure}






\section{\label{sec:conclusion}
Conclusions and Discussions}



In summary, we have presented an adaptive velocity scaling scheme
as a more precise temperature control for the microcanonical ($NVE$) ensemble.
%
Unlike the canonical ($NVT$) ensemble,
the microcanonical ensemble requires a constant total energy, $E$,
which is incompatible with the usual periodic velocity scaling scheme
with a fixed magnitude.
%
Our modified scheme gradually decreases the magnitude of the velocity scaling
by a factor of the number of steps.
%
Like the $1/t$ prescription\cite{
  belardinelli2007, belardinelli2007jcp, belardinelli2008,
  zhou2005, zhou2008, morozov2007}
for the WL algorithm\cite{wang2001, wang2001pre},
the adaptive velocity scaling is able to reach
the desired temperature at an optimal rate.
%
Because of the magnitude control,
the effect of velocity scaling often becomes negligible
at long times so that
a simulation under adaptive velocity scaling
behaves similar to a regular MD simulation.

We may view the adaptive velocity scaling scheme
as an application of the $1/t$ prescription
for adjusting the total energy
to achieve the target temperature.
%
More generally, we may apply this technique
to a pair of correlated variables $x$ and $y$,
to find the optimal value of a parameter, $x$,
that achieves a target value, $y^*$, of an observable, $y$,
on the fly.
%
Then, instead of Eq. \eqref{eq:dE_beta},
we have
$$
\delta x_t = \frac 1 t
\cdot
\frac{ y^* - y_t }
     { y'(x^*) }
.
$$
%
For example, one may determine the optimal move size ($x$)
in a Monte Carlo simulation
in order to reaching a certain acceptance ratio ($y$).
%
We expect this and similar parameter control schemes
to be useful in molecular simulations.




\section{Acknowledgment}

We thank Dr. J. Ma, Dr. Y. Mei, J. Wang,
J. Drake, O. Nassar, Dr. C. Lai, Dr. S. Ou and D. Stuhlsatz
for helpful discussions.
We gratefully acknowledge the Robert A. Welch Foundation (H-0037),
the National Science Foundation (CHE-1152876)
and
the National Institutes of Health (GM-037657) for partial support of this work.
%
%This research used computing resources of the National Science Foundation XSEDE grid.


\appendix


\section{\label{sec:error}
  Alternative derivation of
  the optimal scaling magnitude
  %Eq. \eqref{eq:dE_opt}
}


Here we present an alternative derivation of Eq. \eqref{eq:dE_opt},
which shows that if in each step, we can form an independent correction
to the total energy,
then optimally,
this correction should be used
with a weight of $1/t$.

Let $E_t$ be the total energy at step $t$,
then the aim of velocity scaling is to converge $E_t$
to the desired value of $E^*$.
%
We assume that we can form an independent estimate of $E^*$
in step $t$, $\mathcal E_t$,
which means, equivalently, an independent correction
to the current total energy,
$\Delta \mathcal E_t = \mathcal E_t - E_{t-1}$.
%
The correction can be used in velocity scaling
to guide the amount of energy change.
%
If the independent correction were exact,
then velocity scaling needs to be done only once,
with the energy change $\delta E_t = E_t - E_{t-1}$
being identical to the correction.
%
But since in an MD simulation,
the independent correction contains random error,
velocity scaling must be done continually,
and as we shall see,
if we accept the correction with some
optimal time-dependent magnitude, $\alpha(t)$,
as
$\delta E_t = \alpha(t) \, \Delta \mathcal E_t$,
or
%
\begin{equation}
  \delta E_t = \alpha(t) \, \left( \mathcal E_t - E_{t - 1} \right)
  ,
  \label{eq:Eupdate}
\end{equation}
%
Our aim is to minimize
the expected error of the total energy, $E_T$,
at the end of the simulation, $t = T$.

To find the optimal $\alpha(t)$,
we shall switch to the continuous-time framework.
%
We define the error of the total energy at step $t$ as
$x(t) = E_{t-1} - E^*$,
then $\dot x(t) \approx \delta E_t$,
and Eq. \eqref{eq:Eupdate} can be approximated as
%
\begin{equation}
  \dot x(t)
  =
  -\alpha(t) \, x(t) + \alpha(t) \, \xi(t)
  ,
  \label{eq:x_diffeq}
\end{equation}
%
where $\xi(t) \equiv \mathcal E_t - E^*$
denotes the random fluctuation of the independent estimate at step $t$.
%
The solution of Eq. \eqref{eq:x_diffeq} is
%
\begin{equation}
  x(t)
  =
  x(1) \, e^{ -A(t) }
  +
  \int_1^t \dot u_t(\tau) \, \xi(\tau) \, d\tau
  ,
  %\label{eq:x_sol}
  \notag
\end{equation}
%
where
\begin{align*}
  A(t)
  &=
  \int_1^t \alpha(s) \, ds
  ,
\\
  u_t(\tau)
  &= \exp\left(
    -\int_\tau^t \alpha(s) \, ds
  \right)
  .
  \notag
\end{align*}
%
Thus, the average error at the end of simulation $t = T$
is given by
%
\begin{align}
  \left\langle
    x(T)
  \right\rangle
  &=
  \left\langle
    x(1)
  \right\rangle
  \, e^{ -A(t) }
  ,
  \label{eq:abserr}
\end{align}
%
and the square error
can be measured as
%
\begin{align}
  \left\langle
    x^2(T)
  \right\rangle
  &=
  \left\langle
    x^2(1)
  \right\rangle
  \, e^{ -2 \, A(t) }
  \notag \\
  &+
  \int_1^T
  \int_1^T
  \dot u_T(t) \, \dot u_T(t') \,
  \left\langle
    \xi(t) \, \xi(t')
  \right\rangle
  \, dt \, dt'
  .
  \label{eq:err_functional}
\end{align}
%
Below we shall find the optimal functional form of $\alpha(t)$
that minimizes the square error.



%\subsection{\label{sec:whitenoise}
%White-noise approximation
%}

For a very long simulation,
we may approximate $\xi(t)$ as a white noise,
such that
\begin{equation}
  \left\langle \xi(t) \, \xi(t') \right\rangle
  = \Gamma \, \delta(t - t').
  \notag
  %\label{eq:noise_corr}
\end{equation}
%
Note that this is justifiable only
if the simulation length $T$ is much longer than
the autocorrelation time of $\xi(t)$,
as shown in Fig. \ref{fig:errz}.
%
Under this assumption,
Eq. \eqref{eq:err_functional} is simplified as
%
\begin{equation}
  \left\langle
    x^2(T)
  \right\rangle
  =
  \left\langle
    x^2(1)
  \right\rangle
  \, e^{ -2 \, A(t) }
  +
  \Gamma
  \int_1^T
    \dot u_T^2(t) \, dt
  .
  \label{eq:err_functional_wn}
\end{equation}
%
This expression is a functional of $\alpha(t)$,
or equivalently, a functional of $u_T(t)$.
%
To minimize it under a fixed value of $A(T)$,
which implies fixed values of $u_T(1)$ and $u_T(T)$,
we get, from the Euler-Lagrange equation,
$$
\dot u_T(t) = \mathrm{const},
$$
which leads to the solution,
$u_T(t) = (t + t_0) / (T + t_0)$,
with $c$ and $t_0$ being two constants.
%
As a result, we get the optimal scaling magnitude
%
\begin{equation}
  \alpha(t) = \frac{ 1 } { t + t_0 }
  ,
  \notag
  %\label{eq:alpha_opt}
\end{equation}
%
and the error is
%
$$%\begin{equation}
  \left\langle
    x^2(T)
  \right\rangle
  =
  \frac{
    \left\langle x^2(1) \right\rangle
    \, (1 + t_0)^2
    + \Gamma \, (T - 1)
  }
  {
    (T + t_0)^2
  }
  ,
$$%\end{equation}

We can further determine the optimal value of $t_0$ as
$$
t_0 = \frac{ \Gamma } { \left\langle x^2(1) \right\rangle } - 1,
$$
%
with the minimal error being $\Gamma / (T + t_0)$.
%
However, for a long simulation, the influence of $t_0$
is small, and we may approximately use
$$
\alpha(t) \approx \frac 1 t,
$$
which requires Eq. \eqref{eq:dE_opt}.

We can further show that Eq. \eqref{eq:dE_mod}
%corresponds to a modified schedule
%$\alpha(t) = z/t$,
results in an expected error of
\begin{equation}
  \left\langle
  x^2(T)
  \right\rangle
  =
  \frac{ \langle x^2(1) \rangle } { T^{2z} }
  +
  \frac{ \Gamma \, z^2 } { 2 \, z - 1 }
  \frac{
    T^{2 \, z - 1} - 1
  }
  {
    T^{2 \, z}
  }
  ,
  \notag
  %\label{eq:err_zovert}
\end{equation}
which is minimal at $z = 1$ for a long simulation.

\note{
  To determine the value of $\Gamma$ of an unknown system,
  we may conduct a simulation
  with the scaling magnitude of velocity scaling
  being a constant, $\alpha(t) = \alpha_0$.
  %
  From Eq. \eqref{eq:err_functional_wn}, we find that
  the error
  $\left\langle
    x^2(T)
  \right\rangle
  =
  \Gamma \, \alpha_0 / 2$
  is independent of time $T$.
  Thus, we can compute $\Gamma$ from the trajectory average
  after equilibration,
  \begin{equation}
  \Gamma
  =
  \frac{ 2 } { \alpha_0 } \,
  \overline{
    \bigl(
      E - \overline E
    \bigr)^2
  }
  .
  \notag
  %\label{eq:Gamma_alpha0}
  \end{equation}
}

\section{Microcanonical temperature and its derivative}


Here we derive Eqs. \eqref{eq:betaE_invK} and \eqref{eq:dbetadE}.
We start by introducing a vector field, $\mathbf u$,
in the phase space,
such that
%
\begin{equation}
  \mathbf u \cdot \nabla H = 1
  ,
  \label{eq:unormalization}
\end{equation}
%
anywhere, then
%
\begin{align}
  \Omega'(E)
  &= -\int \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= -\int (\mathbf u \cdot \nabla H) \,
           \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= -\int \mathbf u \cdot
           \nabla \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
     \notag \\
  &= \int \nabla \cdot \mathbf u \,
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  .
  \notag
  %\label{eq:dOmegaE}
\end{align}
%
where we have integrated by parts in the last step.
%
Thus, we have from Eq. \eqref{eq:betaE_def} that
%
\begin{equation}
  \beta(E)
  =
  \left\langle
    \nabla \cdot \mathbf u
  \right\rangle_E
  .
  \label{eq:betaE_average}
\end{equation}
%
Although the vector field $\mathbf u$ may involve both coordinates and momenta,
a convenient choice of $\mathbf u$ that satisfies
Eq. \eqref{eq:unormalization}
is one derived entirely from the momenta
%
\begin{equation}
  \mathbf u
  =
  \frac{ \mathbf p }
       {  2 \, K }
  ,
  \label{eq:u_def}
\end{equation}
where $\mathbf p$ is the momenta vector in the phase space
with the components for coordinates being zeroes,
and $K = \frac 1 2 \mathbf p \cdot \mathbf M^{-1} \cdot \mathbf p$
is the kinetic energy ($\mathbf M$ is the diagonal mass matrix).
%
Equation \eqref{eq:unormalization} is satisfied
because
$\mathbf u \cdot \nabla H
= (\mathbf p \cdot \nabla_{\mathbf p} K)/(2 \, K)
= \mathbf p \cdot (\mathbf M^{-1} \cdot \mathbf p) / (2 \, K) = 1$.
%
Using Eq. \eqref{eq:u_def} in Eq. \eqref{eq:betaE_average}
yields Eq. \eqref{eq:betaE_invK}.

In a similar manner, we have
%
\begin{align*}
  \frac
  {
    d \beta(E)
  }
  {
    d E
  }
  =
  \frac
  {
    \Omega''(E)
  }
  {
    \Omega(E)
  }
  -
  \left[
    \frac
    {
      \Omega'(E)
    }
    {
      \Omega(E)
    }
  \right]^2
  ,
\end{align*}
%
and since
%
\begin{align*}
  \Omega''(E)
  &= -\int \nabla \cdot \mathbf u \,
     \delta'\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  \\
  &= \int
     \nabla \cdot \bigl( (\nabla \cdot \mathbf u) \, \mathbf u \bigr) \,
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  \\
  &= \int
     \left[
     \mathbf u \cdot \nabla (\nabla \cdot \mathbf u)
     +
     (\nabla \cdot \mathbf u)^2
     \right]
     \delta\bigl( H(\mathbf x) - E \bigr) \, d\mathbf x
  ,
\end{align*}
%
We have
%
\begin{align*}
  \beta'(E)
  =
  \left\langle
     \mathbf u \cdot \nabla (\nabla \cdot \mathbf u)
  \right\rangle_E
  +
  \left\langle
    \Delta (\nabla \cdot \mathbf u)^2
  \right\rangle_E
  .
\end{align*}
%
With Eq. \eqref{eq:u_def},
this yields Eq. \eqref{eq:dbetadE}.

Note that in the above derivation,
we have, for simplicity, ignored the conservation
of total momenta\cite{shirts2006, uline2008},
which gives additional constraints satisfied in a typical MD simulation.
%
Fortunately, with the particular choice given by Eq. \eqref{eq:u_def},
one can show that this effect is negligible for a system
with zero total momenta\cite{uline2008}
\big[a key observation here is
that for the total momenta $P_\nu = \sum_{i} p_{i, \nu}$,
$\nu = x,y,z$, we have
$\mathbf u \cdot \nabla P_\nu = P_\nu/(2K) = 0$ \big].



\section{\label{sec:block}
Block average}



If block average is used, we have
%
\begin{align*}
  \sum_{t = 1}^T \frac{ A_t } { t }
  &=
  \sum_{\tau = 1}^{T/m}
  \sum_{\tau' = 1}^{m}
  \frac{ A_t } { (\tau - 1) \, m + \tau' }
  \\
  &\approx
  \sum_{\tau = 1}^{T/m}
  \frac{ 1 } { \tau }
  \left(
    \frac 1 m
    \sum_{\tau' = 1}^m
    A_t
  \right)
  =
  \sum_{\tau = 1}^{T/m}
  \frac{ A^*_\tau } { \tau }
  .
\end{align*}
%
where
$t = (\tau - 1) \, m + \tau'$
and
$
A^*_\tau = \frac 1 m
\sum_{\tau' = 1}^m A_t.
$

In our case,
$
A_t = \beta^* - \frac{ N_f - 2 } { 2 \, K_t }.
$



%\bibliographystyle{abbrv}
\bibliography{simul}
\end{document}
